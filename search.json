[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/16BHW3/index.html",
    "href": "posts/16BHW3/index.html",
    "title": "PIC16B HW3",
    "section": "",
    "text": "In this tutorial, we are going to build a simple message bank with flask fundamentals, database skills, and basic CSS. The app we build should be able to:  1&gt; Allow the user to submit messages to the bank  2&gt; Allow the viewer to view a sample of the messages currently stored in the bank\n\n\n\n\n\nLet’s start with creating a collection of html files in our project folder templates  Template Reference: mnist demo & simple form demo View my html files folder: myhtmls 1&gt; base.html  This file should exhibit a header of the website “A Simple Message Bank” with two navigation links to message submission and viewing 2&gt; submit.html (should extend) base.html  This file should contain: 1) Two text boxes for user input 2) “Submit” button to submit the message to the message bank  3&gt; view.html (should extend) base.html  This file should contain a board that exhibits messages randomly selected for viewing  4&gt; hello.html (should extend) base.html  This file contains a greeting page to users\nNow we need to create a app.py file to write some codes:\n\n\nThis function creates the database of messages messages_db, a table messages, and return the connection g.message.db\n\n# Database connection\ndef get_message_db():\n    try:\n        return g.message_db\n    except AttributeError:\n        # Connect to the database message_db, ensuring the connection is an attribute of g.\n        g.message_db = sqlite3.connect(\"messages_db.sqlite\")\n        # This SQL Command is to first check whether a table called messages exists in the created database\n        # And then it creates a table which includes two text columns (handle and message)\n        cmd = '''\n        CREATE TABLE IF NOT EXISTS messages (\n            id INTEGER PRIMARY KEY AUTOINCREMENT,\n            handle TEXT NOT NULL,\n            message TEXT NOT NULL\n        )\n        '''\n        cursor = g.message_db.cursor()\n        cursor.execute(cmd)\n        g.message_db.commit()\n        # Return the connection g.message_db\n        return g.message_db\n\n\n\n\nThis function handles inserting a user messgae into the dabase of messages\n\n# Insert values into database\ndef insert_message(request):\n    #This SQL command insert both handle and message into two corresponding columns in the table\n    cmd = 'INSERT INTO messages (handle, message) VALUES (?, ?)'\n    #Here I specify the column name of handle as \"name\"\n    handle = request.form[\"name\"]\n    message = request.form[\"message\"]\n    db = get_message_db()\n    #Using cursor to insert the message into message database\n    cursor = db.cursor()\n    #We need provide the handle, the message itself.\n    cursor.execute(cmd, (handle, message))\n    #When working directly with SQL commands, it's neccessary to run db.commit() after inserting a row into db\n    #This is to ensure row insertion has been saved\n    g.message_db.commit()\n    \n\n\n\n\nThis function ensure transmit and receive data, supporting both POST and GET methods\n\n# Submit message route\n# 'POST' and 'GET' splites two returned pages. Ensure the page both transmit and receive data.\n@app.route('/submit', methods=['POST', 'GET'])\n\ndef submit():\n    # In POST case, call insert_message () and if it runs successfuly, render with a \"Thank you\" note\n    if request.method == 'POST':\n        insert_message(request)\n        flash('Thank you for your submission!')\n        return redirect(url_for('view'))\n    else:\n        return render_template('submit.html')\n\n\n\n\n\n\n\nThis function should returns to a collection of n random messages from the database\n\ndef random_messages(n):\n    db_connection = get_message_db()\n    # The cursor is used to execute SQL command and process with returned results\n    cursor = db_connection.cursor()\n    # In this SQL command, 'ORDER BY RANDOM' ensures the result is random, and 'LIMIT' ensures a predefined n number of returned messages\n    cmd = 'SELECT handle, message FROM messages ORDER BY RANDOM() LIMIT ?;'\n    # fetchall() is to get all matched records\n    result = cursor.execute(cmd, (n,)).fetchall()\n    return result \n#This function should return a result of a list, each contains handle and message\n\n\n\n\nThis function first call random_mesages to grab some random messages and pass these messages as an argument to render_template\n\ndef view():\n    #The function returns 5 rows of handle and message\n    result = random_messages(5)\n    # message=results passes the randome messages obtained from the database to the template\n    return render_template('view.html',messages=result)\n\n\n\n\n\n\n# Home route: Directs to the main page. Renders 'hello.html'\n#def main():\n@app.route('/')\ndef home():\n    return render_template('hello.html')\n\n# Hello route: Directs to a page using the '/hello' URL. Also renders \"hello.html\"\n@app.route('/hello')\ndef hello():\n    return render_template('hello.html')\n\n# Submit message route: Handles from submissions\n# On POST, it processes and inserts the message, then redirects to the 'view' route. \n# On GET, it shows the submission form by rendering 'submit.html'.\n@app.route('/submit', methods=['POST', 'GET'])\ndef submit():\n    if request.method == 'POST':\n        insert_message(request)\n        flash('Thank you for your submission!')\n        return redirect(url_for('view'))\n    else:\n        return render_template('submit.html')\n\n# View Route: Displays a list of messages by fetching a predefined number (e.g., 5) of random messages and rendering them on 'view.html'.    \n@app.route('/view')\ndef view():\n    result = random_messages(5)\n    return render_template('view.html',messages=result)\n\n# Random Message Route: Similar to the view route, show a varying set of 5 random messages each time the '/random' URL is accessed. \n# Renders 'random.html' with the messages.\n@app.route('/random')\ndef show_random_messages():\n    messages = random_messages(5)  # Fetch 5 random messages\n    return render_template('random.html', messages=messages)\n\n\n\n\n\n\nNote: please view the complete collection of html files here: myhtmls Today we only discuss how base.html is designed\n\n\n\nScreen Shot 2024-02-14 at 10.55.41 PM.png\n\n\n\n&lt;!doctype html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n&lt;!-- Linking external CSS from the 'static' folder for styling --&gt;\n  &lt;link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\"&gt;\n  &lt;style&gt;\n    /* Inline CSS for basic styling */\n    body {\n      font-family: Arial, sans-serif; /* Setting the font style */\n      background-color: #f4f4f4; /* Grey background */\n      margin: 0; /* Remove default margin */\n      padding: 0; \n    }\n    nav {\n      background-color: #333; /* Dark background for nav */\n      color: white; /* White text color */\n      padding: 10px; /* Padding around nav content */\n    nav ul {\n      padding: 0;\n      list-style: none; /* No bullet points for list */\n    }\n    nav ul li {\n      display: inline; /* Inline display for list items */\n      margin-right: 10px; /* Space between navigation items */\n    }\n    nav a {\n      color: white;\n      text-decoration: none;\n    }\n    .content {\n      margin: 15px; /* Margin around the content section */\n    }\n  &lt;/style&gt;\n  &lt;!-- Dynamic title with a fallback/static part \" - PIC16B Website\" --&gt;\n  &lt;title&gt;{% block title %}{% endblock %} - PIC16B Website&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n  &lt;!-- Navigation bar --&gt;\n  &lt;nav&gt;\n    &lt;h1&gt;A Simple Message Bank&lt;/h1&gt;\n    &lt;ul&gt;\n      &lt;!-- Navigation links to different routes/pages --&gt;\n      &lt;li&gt;&lt;a href=\"{{ url_for('submit') }}\"&gt;Submit a message&lt;/a&gt;&lt;/li&gt;\n      &lt;li&gt;&lt;a href=\"{{ url_for('view')}}\"&gt;View messages&lt;/a&gt;&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/nav&gt;\n &lt;!-- Main content section --&gt;\n  &lt;section class=\"content\"&gt;\n    &lt;header&gt;\n      &lt;!-- Placeholder for header content, can be overridden in child templates --&gt;\n      {% block header %}{% endblock %}\n    &lt;/header&gt;\n    {% block content %}{% endblock %}\n  &lt;/section&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\n\n\n\n\n\n\nScreen Shot 2024-02-14 at 11.11.05 PM.png\n\n\n\n\n\nScreen Shot 2024-02-14 at 11.27.36 PM.png"
  },
  {
    "objectID": "posts/16BHW3/index.html#instructions-a-step-by-step-guide",
    "href": "posts/16BHW3/index.html#instructions-a-step-by-step-guide",
    "title": "PIC16B HW3",
    "section": "",
    "text": "Let’s start with creating a collection of html files in our project folder templates  Template Reference: mnist demo & simple form demo View my html files folder: myhtmls 1&gt; base.html  This file should exhibit a header of the website “A Simple Message Bank” with two navigation links to message submission and viewing 2&gt; submit.html (should extend) base.html  This file should contain: 1) Two text boxes for user input 2) “Submit” button to submit the message to the message bank  3&gt; view.html (should extend) base.html  This file should contain a board that exhibits messages randomly selected for viewing  4&gt; hello.html (should extend) base.html  This file contains a greeting page to users\nNow we need to create a app.py file to write some codes:\n\n\nThis function creates the database of messages messages_db, a table messages, and return the connection g.message.db\n\n# Database connection\ndef get_message_db():\n    try:\n        return g.message_db\n    except AttributeError:\n        # Connect to the database message_db, ensuring the connection is an attribute of g.\n        g.message_db = sqlite3.connect(\"messages_db.sqlite\")\n        # This SQL Command is to first check whether a table called messages exists in the created database\n        # And then it creates a table which includes two text columns (handle and message)\n        cmd = '''\n        CREATE TABLE IF NOT EXISTS messages (\n            id INTEGER PRIMARY KEY AUTOINCREMENT,\n            handle TEXT NOT NULL,\n            message TEXT NOT NULL\n        )\n        '''\n        cursor = g.message_db.cursor()\n        cursor.execute(cmd)\n        g.message_db.commit()\n        # Return the connection g.message_db\n        return g.message_db\n\n\n\n\nThis function handles inserting a user messgae into the dabase of messages\n\n# Insert values into database\ndef insert_message(request):\n    #This SQL command insert both handle and message into two corresponding columns in the table\n    cmd = 'INSERT INTO messages (handle, message) VALUES (?, ?)'\n    #Here I specify the column name of handle as \"name\"\n    handle = request.form[\"name\"]\n    message = request.form[\"message\"]\n    db = get_message_db()\n    #Using cursor to insert the message into message database\n    cursor = db.cursor()\n    #We need provide the handle, the message itself.\n    cursor.execute(cmd, (handle, message))\n    #When working directly with SQL commands, it's neccessary to run db.commit() after inserting a row into db\n    #This is to ensure row insertion has been saved\n    g.message_db.commit()\n    \n\n\n\n\nThis function ensure transmit and receive data, supporting both POST and GET methods\n\n# Submit message route\n# 'POST' and 'GET' splites two returned pages. Ensure the page both transmit and receive data.\n@app.route('/submit', methods=['POST', 'GET'])\n\ndef submit():\n    # In POST case, call insert_message () and if it runs successfuly, render with a \"Thank you\" note\n    if request.method == 'POST':\n        insert_message(request)\n        flash('Thank you for your submission!')\n        return redirect(url_for('view'))\n    else:\n        return render_template('submit.html')\n\n\n\n\n\n\n\nThis function should returns to a collection of n random messages from the database\n\ndef random_messages(n):\n    db_connection = get_message_db()\n    # The cursor is used to execute SQL command and process with returned results\n    cursor = db_connection.cursor()\n    # In this SQL command, 'ORDER BY RANDOM' ensures the result is random, and 'LIMIT' ensures a predefined n number of returned messages\n    cmd = 'SELECT handle, message FROM messages ORDER BY RANDOM() LIMIT ?;'\n    # fetchall() is to get all matched records\n    result = cursor.execute(cmd, (n,)).fetchall()\n    return result \n#This function should return a result of a list, each contains handle and message\n\n\n\n\nThis function first call random_mesages to grab some random messages and pass these messages as an argument to render_template\n\ndef view():\n    #The function returns 5 rows of handle and message\n    result = random_messages(5)\n    # message=results passes the randome messages obtained from the database to the template\n    return render_template('view.html',messages=result)\n\n\n\n\n\n\n# Home route: Directs to the main page. Renders 'hello.html'\n#def main():\n@app.route('/')\ndef home():\n    return render_template('hello.html')\n\n# Hello route: Directs to a page using the '/hello' URL. Also renders \"hello.html\"\n@app.route('/hello')\ndef hello():\n    return render_template('hello.html')\n\n# Submit message route: Handles from submissions\n# On POST, it processes and inserts the message, then redirects to the 'view' route. \n# On GET, it shows the submission form by rendering 'submit.html'.\n@app.route('/submit', methods=['POST', 'GET'])\ndef submit():\n    if request.method == 'POST':\n        insert_message(request)\n        flash('Thank you for your submission!')\n        return redirect(url_for('view'))\n    else:\n        return render_template('submit.html')\n\n# View Route: Displays a list of messages by fetching a predefined number (e.g., 5) of random messages and rendering them on 'view.html'.    \n@app.route('/view')\ndef view():\n    result = random_messages(5)\n    return render_template('view.html',messages=result)\n\n# Random Message Route: Similar to the view route, show a varying set of 5 random messages each time the '/random' URL is accessed. \n# Renders 'random.html' with the messages.\n@app.route('/random')\ndef show_random_messages():\n    messages = random_messages(5)  # Fetch 5 random messages\n    return render_template('random.html', messages=messages)\n\n\n\n\n\n\nNote: please view the complete collection of html files here: myhtmls Today we only discuss how base.html is designed\n\n\n\nScreen Shot 2024-02-14 at 10.55.41 PM.png\n\n\n\n&lt;!doctype html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n&lt;!-- Linking external CSS from the 'static' folder for styling --&gt;\n  &lt;link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\"&gt;\n  &lt;style&gt;\n    /* Inline CSS for basic styling */\n    body {\n      font-family: Arial, sans-serif; /* Setting the font style */\n      background-color: #f4f4f4; /* Grey background */\n      margin: 0; /* Remove default margin */\n      padding: 0; \n    }\n    nav {\n      background-color: #333; /* Dark background for nav */\n      color: white; /* White text color */\n      padding: 10px; /* Padding around nav content */\n    nav ul {\n      padding: 0;\n      list-style: none; /* No bullet points for list */\n    }\n    nav ul li {\n      display: inline; /* Inline display for list items */\n      margin-right: 10px; /* Space between navigation items */\n    }\n    nav a {\n      color: white;\n      text-decoration: none;\n    }\n    .content {\n      margin: 15px; /* Margin around the content section */\n    }\n  &lt;/style&gt;\n  &lt;!-- Dynamic title with a fallback/static part \" - PIC16B Website\" --&gt;\n  &lt;title&gt;{% block title %}{% endblock %} - PIC16B Website&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n  &lt;!-- Navigation bar --&gt;\n  &lt;nav&gt;\n    &lt;h1&gt;A Simple Message Bank&lt;/h1&gt;\n    &lt;ul&gt;\n      &lt;!-- Navigation links to different routes/pages --&gt;\n      &lt;li&gt;&lt;a href=\"{{ url_for('submit') }}\"&gt;Submit a message&lt;/a&gt;&lt;/li&gt;\n      &lt;li&gt;&lt;a href=\"{{ url_for('view')}}\"&gt;View messages&lt;/a&gt;&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/nav&gt;\n &lt;!-- Main content section --&gt;\n  &lt;section class=\"content\"&gt;\n    &lt;header&gt;\n      &lt;!-- Placeholder for header content, can be overridden in child templates --&gt;\n      {% block header %}{% endblock %}\n    &lt;/header&gt;\n    {% block content %}{% endblock %}\n  &lt;/section&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\n\n\n\n\n\n\nScreen Shot 2024-02-14 at 11.11.05 PM.png\n\n\n\n\n\nScreen Shot 2024-02-14 at 11.27.36 PM.png"
  },
  {
    "objectID": "posts/16BHW5/index.html",
    "href": "posts/16BHW5/index.html",
    "title": "PIC16B HW5",
    "section": "",
    "text": "Tutorial 5: Image Classification in Keras through Tensorflow Datasets\nThis blog tutorial will present different approaches to conduct image classification in Keras with data fed through Tensorflow Datasets.\n\n\nBackground Knowledge:\n· Keras: Keras 3 functions as an independent library capable of interfacing with various computational backends, including TensorFlow, PyTorch, and JAX, offering flexibility in choosing the underlying framework for deep learning projects.\n· TensorFlow Datasets: This feature offers a streamlined approach to managing datasets for training, validation, and testing purposes, enhancing the efficiency of data handling in machine learning workflows.\n· Data augmentation: Through data augmentation, we can generate additional variations of our dataset, enhancing the model’s ability to learn and generalize from diverse patterns more effectively.\n· Transfer learning: Transfer learning uses the knowledge gained from previously trained models to tackle new, related problems, saving time and computational resources during model development.\n\n\nInstructions:\n1.Load Packages and Obtain Data\nWe firstly include a block that includes all import statements with a brief introduction of their implications:\n\nimport os # 'os' allows interaction wth the operation system\nos.environ[\"KERAS_BACKEND\"] = \"torch\"\n\nfrom keras import utils # 'utils' contains utilities for data processing and sequence propocessing\nimport tensorflow_datasets as tfds # TFDS library is a collection of dataset to use with TensorFlow\nimport matplotlib.pyplot as plt # 'pyplot' in 'matplotlib' library creates data visualization\nimport numpy as np # 'numpy' supports mathematical operations\n\nfrom random import randint # 'randit' generates randome integers with a specified range\n\nimport keras # 'keras' supports deep neural networks experimentation\nfrom keras import layers # 'layers' is building blocks of networks in Keras, contains functions like 'Dense', 'Conv2D', etc.\n\nBy utilizing Kaggle, we gain access to a dataset featuring labeled images of cats and dogs. By executing the code below, we successfully establish datasets designated for training, validation, and testing purposes.\n\ntrain_ds, validation_ds, test_ds = tfds.load(\n    \"cats_vs_dogs\",\n    # 40% for training, 10% for validation, and 10% for test (the rest unused)\n    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],\n    as_supervised=True,  # Include labels\n)\n\nprint(f\"Number of training samples: {train_ds.cardinality()}\")\nprint(f\"Number of validation samples: {validation_ds.cardinality()}\")\nprint(f\"Number of test samples: {test_ds.cardinality()}\")\n\nNumber of training samples: 9305\nNumber of validation samples: 2326\nNumber of test samples: 2326\n\n\nWe use the following code to resize all different sized images in datasets to a fixed size of 150x150\n\nresize_fn = keras.layers.Resizing(150, 150)\n\ntrain_ds = train_ds.map(lambda x, y: (resize_fn(x), y))\nvalidation_ds = validation_ds.map(lambda x, y: (resize_fn(x), y))\ntest_ds = test_ds.map(lambda x, y: (resize_fn(x), y))\n\nThe next block uses batch_size to determine “how many data points are gathered from the directory”:\n\nfrom tensorflow import data as tf_data\nbatch_size = 64\n\ntrain_ds = train_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\nvalidation_ds = validation_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\ntest_ds = test_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\n\nWe now write a function to create two-row visualization with one row of cats and one row of dogs\n\nclass_names = ['cat', 'dog']\n\ndef visualize(ds):\n  plt.figure(figsize=(10, 10)) #figure size\n  for images, labels in ds.take(1):  # Take one batch at this step\n        cat_count = 0\n        dog_count = 0\n        while cat_count &lt; 3 or dog_count &lt; 3:\n            i = randint(0, 63)  # We before set batch size to 64\n            if cat_count &lt; 3 and labels[i] == 0:  # If we need more cats and found one\n                ax = plt.subplot(2, 3, cat_count + 1) # plot the cat image\n                plt.imshow(images[i].numpy().astype(\"uint8\")) # show the image\n                plt.title(class_names[labels[i]]) # set the title of images\n                cat_count += 1 # increment the cat counter\n            elif dog_count &lt; 3 and labels[i] == 1:  # If we need more dogs and found one\n                ax = plt.subplot(2, 3, dog_count + 4)\n                plt.imshow(images[i].numpy().astype(\"uint8\"))\n                plt.title(class_names[labels[i]])\n                dog_count += 1\n            if cat_count == 3 and dog_count == 3:\n                break  # Exit once 3 cats and 3 dogs have been displayed\n  plt.tight_layout() # adjust layout\n  plt.show()\n\nvisualize(train_ds)# Update the function call\n\n\n\n\n\n\n\n\nThe following code will create an iterator called labels_iterator:\nlabels_iterator= train_ds.unbatch().map(lambda image, label: label).as_numpy_iterator()\nWe will compute the number of image in the training set with label 0 (“cat”) and label 1 (“dog”). In the baseline machine learning model, we will examine how accurate it works in our case. Baseline model will be our benchmark for improvement.\n\nlabels_iterator = train_ds.unbatch().map(lambda image, label: label).as_numpy_iterator()\n\n# Initialize counters for each label\nlabel_0_cat = 0\nlabel_1_dog = 0\n\nfor label in labels_iterator: # iterate through the labels_iterator to count labels\n    if label == 0:\n        label_0_cat += 1 # add counts\n    elif label == 1:\n        label_1_dog += 1\n\nprint(f\"Number of 'cat' images (label 0): {label_0_cat}\")\nprint(f\"Number of 'dog' images (label 1): {label_1_dog}\")\n\nNumber of 'cat' images (label 0): 4637\nNumber of 'dog' images (label 1): 4668\n\n\n4668 &gt; 4637, the baseline accuracy is calculated by “the most frequent label”/ “total label”; In this case, it is calculated by dog/(dog+cat)\n\nprint(\"The baseline accuracy is\", label_1_dog/(label_1_dog+label_0_cat))\n\nThe baseline accuracy is 0.5016657710908113\n\n\n2. Model 1\n“The simplest way to make a model is by using the keras.models.Sequential API, which allows you to construct a model by simply passing a list of layers.”\nLet’s start to construct model 1 (a keras.Sequential model) using at least two Conv2D layers, at least two MaxPooling2D layers, at least one Flatten layer, at least one Dense layer, and at least one Dropout layer.\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\n\"\"\"\nConv2D layer: creates convolution kernal, produces a tensor of outputs\nMaxPooling 2D layer: reduces the dimensionality of the featured maps\nFlatten layer: Converts 2D feature maps into 1D feature vector\nDropout layer: reduces overfitting by dropping a portion of the input\nDense layer: performs classification based on processed previous layers\nReLu: applied element-wise, directly output positive numbers, if negative, output zero,\n\"\"\"\nmodel1 = keras.models.Sequential([\n    # define shape:\n    layers.Input((150,150,3)), #the shape indicates the model input images 150 pixesl in height, 150 pixels in width, 3 channels (RGB color channels)\n    layers.Dropout(0.3), # randomly drop out 30% of the input\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    #learning a total of 32 filters\n    layers.MaxPooling2D((2, 2)),\n    #use Max Pooling to reduce the spatial dimensions of the output volume\n    layers.Conv2D(64, (3, 3), activation='relu'), #relu: recitified linear unit\n    layers.MaxPooling2D((2, 2)),\n    layers.Flatten(), #flattens the input, makes the multidimensional input 1D\n    layers.Dense(64, activation='relu'),\n    layers.Dense(64, activation='sigmoid') # sigmoid for the cat/dog binary classification\n\n])\nmodel1.summary()\n\nModel: \"sequential_39\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dropout_33 (Dropout)        (None, 150, 150, 3)       0         \n                                                                 \n conv2d_70 (Conv2D)          (None, 148, 148, 32)      896       \n                                                                 \n max_pooling2d_64 (MaxPooli  (None, 74, 74, 32)        0         \n ng2D)                                                           \n                                                                 \n conv2d_71 (Conv2D)          (None, 72, 72, 64)        18496     \n                                                                 \n max_pooling2d_65 (MaxPooli  (None, 36, 36, 64)        0         \n ng2D)                                                           \n                                                                 \n flatten_39 (Flatten)        (None, 82944)             0         \n                                                                 \n dense_72 (Dense)            (None, 64)                5308480   \n                                                                 \n dense_73 (Dense)            (None, 64)                4160      \n                                                                 \n=================================================================\nTotal params: 5332032 (20.34 MB)\nTrainable params: 5332032 (20.34 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\nWe first complile the model and then train the model\n\nmodel1.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\nhistory = model1.fit(train_ds, epochs=20, validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 16s 75ms/step - loss: 15.2972 - accuracy: 0.5080 - val_loss: 0.6908 - val_accuracy: 0.5821\nEpoch 2/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.7036 - accuracy: 0.6230 - val_loss: 0.6735 - val_accuracy: 0.5997\nEpoch 3/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.5900 - accuracy: 0.6874 - val_loss: 0.7037 - val_accuracy: 0.5924\nEpoch 4/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.5278 - accuracy: 0.7368 - val_loss: 0.7068 - val_accuracy: 0.6135\nEpoch 5/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.4638 - accuracy: 0.7823 - val_loss: 0.7622 - val_accuracy: 0.6144\nEpoch 6/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.4135 - accuracy: 0.8124 - val_loss: 0.8646 - val_accuracy: 0.6045\nEpoch 7/20\n146/146 [==============================] - 5s 34ms/step - loss: 0.4326 - accuracy: 0.8048 - val_loss: 0.9029 - val_accuracy: 0.6230\nEpoch 8/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.4031 - accuracy: 0.8211 - val_loss: 0.8811 - val_accuracy: 0.6333\nEpoch 9/20\n146/146 [==============================] - 5s 34ms/step - loss: 0.3462 - accuracy: 0.8520 - val_loss: 0.9374 - val_accuracy: 0.6148\nEpoch 10/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.3159 - accuracy: 0.8674 - val_loss: 0.9894 - val_accuracy: 0.6118\nEpoch 11/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.2923 - accuracy: 0.8792 - val_loss: 1.1118 - val_accuracy: 0.6156\nEpoch 12/20\n146/146 [==============================] - 5s 34ms/step - loss: 0.2548 - accuracy: 0.8966 - val_loss: 1.2389 - val_accuracy: 0.6208\nEpoch 13/20\n146/146 [==============================] - 5s 31ms/step - loss: 0.2217 - accuracy: 0.9104 - val_loss: 1.1945 - val_accuracy: 0.6341\nEpoch 14/20\n146/146 [==============================] - 5s 31ms/step - loss: 0.1966 - accuracy: 0.9164 - val_loss: 1.3416 - val_accuracy: 0.6367\nEpoch 15/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.1637 - accuracy: 0.9322 - val_loss: 1.4218 - val_accuracy: 0.6161\nEpoch 16/20\n146/146 [==============================] - 5s 31ms/step - loss: 0.1591 - accuracy: 0.9382 - val_loss: 1.4969 - val_accuracy: 0.6225\nEpoch 17/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.1623 - accuracy: 0.9344 - val_loss: 1.4675 - val_accuracy: 0.6466\nEpoch 18/20\n146/146 [==============================] - 5s 31ms/step - loss: 0.1283 - accuracy: 0.9515 - val_loss: 1.6020 - val_accuracy: 0.6410\nEpoch 19/20\n146/146 [==============================] - 5s 31ms/step - loss: 0.1174 - accuracy: 0.9562 - val_loss: 1.6733 - val_accuracy: 0.6260\nEpoch 20/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.1317 - accuracy: 0.9500 - val_loss: 1.6680 - val_accuracy: 0.6475\n\n\n\nimport matplotlib.pyplot as plt\n\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.ylim([0, 1])\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\nplt.show()\n\n\n\n\n\n\n\n\nValidation Accuracy: The accuracy of my model stabilized between 59% and 64% during training.\nComparison to Baseline: Compare to the baseline accuracy 50%, there is a 10% improvement.\nOverfitting Observation: I observe an overfitting in model 1. This suggests the model 1 cannot fits too closely to the training dataset.\n3.Model 2: Model with Data Augmentation\nIn the model2, we’re incorporating layers that perform data augmentation. This technique involves adding altered versions of existing images to the training dataset. Even when an image is rotated/fliped to a certain degree, it remains itself. By including these rotated or “flipped” images in our training process, we aim to enable the model to identify and understand the “unchanging characteristics” of the input images\nStep 1: we first create a keras.layers.RandomFlip() layer, plotting the original image and some copies to show the application of RandomFlip()\n\n# Select an example from the training dataset\nfor images, _ in train_ds.take(1):\n    image = images[0]\n    break\n\n# Create a keras.layers.RandomFlip()layer\nrandom_flip = tf.keras.layers.RandomFlip(\"horizontal_and_vertical\")\n# flip radnomly in vertical and horizontal directions\n\n# flip randomly for serveral times and plot images\nplt.figure(figsize=(10, 2))  # set image size\nplt.subplot(1, 5, 1)  # original image\nplt.imshow(image.numpy().astype(\"uint8\"))\nplt.title(\"Original\")\nfor i in range(2, 6):  # flip four times\n    flipped_image = random_flip(image, training=True)\n    plt.subplot(1, 5, i)\n    plt.imshow(flipped_image.numpy().astype(\"uint8\"))\n    plt.title(f\"Flipped {i-1}\")  # plot flipped images with corresponding titles\nplt.show()\n\n\n\n\n\n\n\n\nStep 2: we then create keras.layers.RandomRotation layer. Then, we make a plot of both the original image and a few copies to which RandomRotation() has been applied.\n\n# create a keras.layers.RandomRotation, set random rotation factor as 0.24\nrandom_rotation = tf.keras.layers.RandomRotation(0.24)\n\nplt.figure(figsize=(10, 2))  # set size\nplt.subplot(1, 5, 1)\nplt.imshow(image.numpy().astype(\"uint8\"))\nplt.title(\"Original\")\nfor i in range(2, 6):  # rotates randomly four times\n    rotated_image = random_rotation(image, training=True)\n    plt.subplot(1, 5, i)\n    plt.imshow(rotated_image.numpy().astype(\"uint8\"))\n    plt.title(f\"Rotated {i-1}\")  # plot rotatated images\nplt.show()\n\n\n\n\n\n\n\n\nStep 3: we then create a new keras.models.Sequential model called model2 in which “the first two layers are augmentation layers” with the usage of a RandomFlip() layer and a RandomRotation() layer.\n\nmodel2 = keras.models.Sequential([\n    layers.Input((150,150,3)),\n    layers.RandomFlip(\"horizontal_and_vertical\"),\n    layers.RandomRotation(0.24),\n    layers.Conv2D(32, (3, 3), activation='relu'),#learning a total of 32 filters\n    layers.MaxPooling2D((2, 2)),\n    #use Max Pooling to reduce the spatial dimensions of the output volume\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Flatten(), #flattens the input, makes the multidimensional input 1D\n    layers.Dense(64, activation='relu'),\n    layers.Dropout(0.3), # randomly drop out 30% of the input\n    layers.Dense(64, activation='softmax') # normalize the outputs, convert from weighted sum values into probabilities that sum to one\n\n])\nmodel2.summary()\n\nModel: \"sequential_9\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n random_flip_8 (RandomFlip)  (None, 150, 150, 3)       0         \n                                                                 \n random_rotation_8 (RandomR  (None, 150, 150, 3)       0         \n otation)                                                        \n                                                                 \n conv2d_18 (Conv2D)          (None, 148, 148, 32)      896       \n                                                                 \n max_pooling2d_18 (MaxPooli  (None, 74, 74, 32)        0         \n ng2D)                                                           \n                                                                 \n conv2d_19 (Conv2D)          (None, 72, 72, 64)        18496     \n                                                                 \n max_pooling2d_19 (MaxPooli  (None, 36, 36, 64)        0         \n ng2D)                                                           \n                                                                 \n flatten_9 (Flatten)         (None, 82944)             0         \n                                                                 \n dense_18 (Dense)            (None, 64)                5308480   \n                                                                 \n dropout_9 (Dropout)         (None, 64)                0         \n                                                                 \n dense_19 (Dense)            (None, 64)                4160      \n                                                                 \n=================================================================\nTotal params: 5332032 (20.34 MB)\nTrainable params: 5332032 (20.34 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\nmodel2.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\nhistory = model2.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 7s 36ms/step - loss: 0.5868 - accuracy: 0.6992 - val_loss: 0.5566 - val_accuracy: 0.7184\nEpoch 2/20\n146/146 [==============================] - 5s 34ms/step - loss: 0.5708 - accuracy: 0.7112 - val_loss: 0.5493 - val_accuracy: 0.7244\nEpoch 3/20\n146/146 [==============================] - 5s 34ms/step - loss: 0.5708 - accuracy: 0.7065 - val_loss: 0.5606 - val_accuracy: 0.7240\nEpoch 4/20\n146/146 [==============================] - 5s 34ms/step - loss: 0.5692 - accuracy: 0.7074 - val_loss: 0.5529 - val_accuracy: 0.7214\nEpoch 5/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.5632 - accuracy: 0.7189 - val_loss: 0.5474 - val_accuracy: 0.7266\nEpoch 6/20\n146/146 [==============================] - 5s 34ms/step - loss: 0.5658 - accuracy: 0.7091 - val_loss: 0.5465 - val_accuracy: 0.7240\nEpoch 7/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.5518 - accuracy: 0.7175 - val_loss: 0.5333 - val_accuracy: 0.7347\nEpoch 8/20\n146/146 [==============================] - 5s 34ms/step - loss: 0.5664 - accuracy: 0.7088 - val_loss: 0.5502 - val_accuracy: 0.7188\nEpoch 9/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.5593 - accuracy: 0.7157 - val_loss: 0.5410 - val_accuracy: 0.7309\nEpoch 10/20\n146/146 [==============================] - 5s 34ms/step - loss: 0.5597 - accuracy: 0.7113 - val_loss: 0.5422 - val_accuracy: 0.7296\nEpoch 11/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.5496 - accuracy: 0.7244 - val_loss: 0.5453 - val_accuracy: 0.7188\nEpoch 12/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.5475 - accuracy: 0.7282 - val_loss: 0.5550 - val_accuracy: 0.7145\nEpoch 13/20\n146/146 [==============================] - 5s 34ms/step - loss: 0.5682 - accuracy: 0.7145 - val_loss: 0.5440 - val_accuracy: 0.7253\nEpoch 14/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.5588 - accuracy: 0.7091 - val_loss: 0.5410 - val_accuracy: 0.7309\nEpoch 15/20\n146/146 [==============================] - 5s 34ms/step - loss: 0.5571 - accuracy: 0.7234 - val_loss: 0.5609 - val_accuracy: 0.7223\nEpoch 16/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.5517 - accuracy: 0.7222 - val_loss: 0.5586 - val_accuracy: 0.7180\nEpoch 17/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.5535 - accuracy: 0.7207 - val_loss: 0.5363 - val_accuracy: 0.7283\nEpoch 18/20\n146/146 [==============================] - 5s 34ms/step - loss: 0.5566 - accuracy: 0.7175 - val_loss: 0.5341 - val_accuracy: 0.7386\nEpoch 19/20\n146/146 [==============================] - 5s 34ms/step - loss: 0.5469 - accuracy: 0.7258 - val_loss: 0.5296 - val_accuracy: 0.7360\nEpoch 20/20\n146/146 [==============================] - 5s 34ms/step - loss: 0.5519 - accuracy: 0.7183 - val_loss: 0.5311 - val_accuracy: 0.7304\n\n\n\nimport matplotlib.pyplot as plt\n\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.ylim([0, 1])\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\nplt.show()\n\n\n\n\n\n\n\n\nValidation Accuracy: The accuracy of my model stabilized between 69% and 73% during training.\nComparison to model1: Compare to the model1 59%-64% accuracy, there is an around 9% improvement.\nOverfitting Observation: I observe little overfitting in model 2. This suggests the model2 fits coser than model1 to the training dataset. cool.\n4.Model 3: Model with Data Preprocessing\nBy normalizing RGB values from between 0 to 255 to between 0 and 1 (or possibly between -1 and 1), many models can be trained faster. If we handle weight scaling prior to the training proess, we can spend more traning energy handling actual signal and have the weights adjust to the data scale.\nThe following code creates a preprocessing layer called preprocessor which can be slot into our model pipeline:\n\ni = keras.Input(shape=(150, 150, 3))\n# The pixel values have the range of (0, 255), but many models will work better if rescaled to (-1, 1.)\n# outputs: `(inputs * scale) + offset`\nscale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\nx = scale_layer(i)\npreprocessor = keras.Model(inputs = i, outputs = x)\n\n\nmodel3 = keras.models.Sequential([\n    layers.Input((150,150,3)),\n    preprocessor,\n    layers.RandomFlip(\"horizontal_and_vertical\"),\n    layers.RandomRotation(0.24),\n    layers.Conv2D(32, (3, 3), activation='relu'),#learning a total of 32 filters\n    layers.MaxPooling2D((2, 2)),\n    #use Max Pooling to reduce the spatial dimensions of the output volume\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Flatten(), #flattens the input, makes the multidimensional input 1D\n\n    layers.Dropout(0.3), # randomly drop out 30% of the input\n    layers.Dense(64, activation='softmax') # normalize the outputs, convert from weighted sum values into probabilities that sum to one\n\n])\nmodel3.summary()\n\nModel: \"sequential_40\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n model_5 (Functional)        (None, 150, 150, 3)       0         \n                                                                 \n random_flip_36 (RandomFlip  (None, 150, 150, 3)       0         \n )                                                               \n                                                                 \n random_rotation_36 (Random  (None, 150, 150, 3)       0         \n Rotation)                                                       \n                                                                 \n conv2d_72 (Conv2D)          (None, 148, 148, 32)      896       \n                                                                 \n max_pooling2d_66 (MaxPooli  (None, 74, 74, 32)        0         \n ng2D)                                                           \n                                                                 \n conv2d_73 (Conv2D)          (None, 72, 72, 64)        18496     \n                                                                 \n max_pooling2d_67 (MaxPooli  (None, 36, 36, 64)        0         \n ng2D)                                                           \n                                                                 \n flatten_40 (Flatten)        (None, 82944)             0         \n                                                                 \n dropout_34 (Dropout)        (None, 82944)             0         \n                                                                 \n dense_74 (Dense)            (None, 64)                5308480   \n                                                                 \n=================================================================\nTotal params: 5327872 (20.32 MB)\nTrainable params: 5327872 (20.32 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\nmodel3.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\nhistory = model3.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 116s 40ms/step - loss: 0.3754 - accuracy: 0.8347 - val_loss: 0.4780 - val_accuracy: 0.8147\nEpoch 2/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.3760 - accuracy: 0.8321 - val_loss: 0.4999 - val_accuracy: 0.8091\nEpoch 3/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.3707 - accuracy: 0.8374 - val_loss: 0.4653 - val_accuracy: 0.8138\nEpoch 4/20\n146/146 [==============================] - 5s 36ms/step - loss: 0.3797 - accuracy: 0.8280 - val_loss: 0.4800 - val_accuracy: 0.8108\nEpoch 5/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.3780 - accuracy: 0.8320 - val_loss: 0.4570 - val_accuracy: 0.8224\nEpoch 6/20\n146/146 [==============================] - 5s 37ms/step - loss: 0.3760 - accuracy: 0.8352 - val_loss: 0.4634 - val_accuracy: 0.8160\nEpoch 7/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.3765 - accuracy: 0.8345 - val_loss: 0.4479 - val_accuracy: 0.8151\nEpoch 8/20\n146/146 [==============================] - 5s 36ms/step - loss: 0.3763 - accuracy: 0.8321 - val_loss: 0.5011 - val_accuracy: 0.8091\nEpoch 9/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.3742 - accuracy: 0.8321 - val_loss: 0.4702 - val_accuracy: 0.8108\nEpoch 10/20\n146/146 [==============================] - 5s 36ms/step - loss: 0.3705 - accuracy: 0.8334 - val_loss: 0.4505 - val_accuracy: 0.8169\nEpoch 11/20\n146/146 [==============================] - 5s 38ms/step - loss: 0.3709 - accuracy: 0.8354 - val_loss: 0.4823 - val_accuracy: 0.8117\nEpoch 12/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.3748 - accuracy: 0.8328 - val_loss: 0.4942 - val_accuracy: 0.8065\nEpoch 13/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.3720 - accuracy: 0.8325 - val_loss: 0.4717 - val_accuracy: 0.8104\nEpoch 14/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.3700 - accuracy: 0.8348 - val_loss: 0.5093 - val_accuracy: 0.8113\nEpoch 15/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.3726 - accuracy: 0.8347 - val_loss: 0.4981 - val_accuracy: 0.8095\nEpoch 16/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.3694 - accuracy: 0.8329 - val_loss: 0.5493 - val_accuracy: 0.7962\nEpoch 17/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.3650 - accuracy: 0.8408 - val_loss: 0.4625 - val_accuracy: 0.8212\nEpoch 18/20\n146/146 [==============================] - 5s 37ms/step - loss: 0.3723 - accuracy: 0.8357 - val_loss: 0.4866 - val_accuracy: 0.8083\nEpoch 19/20\n146/146 [==============================] - 5s 36ms/step - loss: 0.3674 - accuracy: 0.8414 - val_loss: 0.5071 - val_accuracy: 0.8100\nEpoch 20/20\n146/146 [==============================] - 5s 36ms/step - loss: 0.3610 - accuracy: 0.8402 - val_loss: 0.4767 - val_accuracy: 0.8061\n\n\nException ignored in: &lt;function _xla_gc_callback at 0x7a26d6e763b0&gt;\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/lib/__init__.py\", line 97, in _xla_gc_callback\n    def _xla_gc_callback(*args):\nKeyboardInterrupt: \n\n\n\nimport matplotlib.pyplot as plt\n\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.ylim([0, 1])\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\nplt.show()\n\n\n\n\n\n\n\n\nValidation Accuracy: The accuracy of my model stabilized between 81% and 82% during training.\nComparison to model2: Compare to the model2 68%-72%, there is an around 11% improvement.\nOverfitting Observation: I observe little overfitting in model 3.\n5.Model 4: Model with Transfer Learning\nTo improve our model training, we could also access to pre-existing “base models” and incorporate it into a full model our current project, then continue training.\nWe paste the following code to download MobileNetV3Large and configured it as a layer we could include in our model.\n\nIMG_SHAPE = (150, 150, 3)\nbase_model = keras.applications.MobileNetV3Large(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet')\nbase_model.trainable = False\n\ni = keras.Input(shape=IMG_SHAPE)\nx = base_model(i, training = False)\nbase_model_layer = keras.Model(inputs = i, outputs = x)\n# preprocessing layers are included in MobileNetV3Large.\n\nWARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n\n\n\nmodel4 = keras.models.Sequential([\n    layers.Input((150,150,3)),\n    base_model_layer, # add this layer we defined above\n    # we keep data augmentation layers from Part 3\n    layers.RandomFlip(\"horizontal_and_vertical\"),\n    layers.RandomRotation(0.24),\n    layers.Flatten(), #flattens the input, makes the multidimensional input 1D\n    layers.Dropout(0.3), # I put an additionaly dropout layer; we don't need a lot additional layers\n    layers.Dense(2)# we have Dense(2) layer at the very end to actually perform the classification\n\n])\nmodel4.summary()\n\nModel: \"sequential_35\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n model_7 (Functional)        (None, 5, 5, 960)         2996352   \n                                                                 \n random_flip_34 (RandomFlip  (None, 5, 5, 960)         0         \n )                                                               \n                                                                 \n random_rotation_34 (Random  (None, 5, 5, 960)         0         \n Rotation)                                                       \n                                                                 \n flatten_35 (Flatten)        (None, 24000)             0         \n                                                                 \n dropout_29 (Dropout)        (None, 24000)             0         \n                                                                 \n dense_66 (Dense)            (None, 2)                 48002     \n                                                                 \n=================================================================\nTotal params: 3044354 (11.61 MB)\nTrainable params: 48002 (187.51 KB)\nNon-trainable params: 2996352 (11.43 MB)\n_________________________________________________________________\n\n\n*Model4 Summary Analysis: The base_model_layer is nuanced: There are 48002 trainable parameters in the model 4. 3044354 parameters in total.\n\nmodel4.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\nhistory = model4.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 123s 52ms/step - loss: 0.4829 - accuracy: 0.9542 - val_loss: 0.5246 - val_accuracy: 0.9480\nEpoch 2/20\n146/146 [==============================] - 6s 42ms/step - loss: 0.3985 - accuracy: 0.9549 - val_loss: 0.4381 - val_accuracy: 0.9488\nEpoch 3/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.3666 - accuracy: 0.9531 - val_loss: 0.4337 - val_accuracy: 0.9441\nEpoch 4/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.3244 - accuracy: 0.9344 - val_loss: 0.3844 - val_accuracy: 0.9187\nEpoch 5/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.3517 - accuracy: 0.9459 - val_loss: 0.4143 - val_accuracy: 0.9514\nEpoch 6/20\n146/146 [==============================] - 7s 46ms/step - loss: 0.3307 - accuracy: 0.9553 - val_loss: 0.4066 - val_accuracy: 0.9523\nEpoch 7/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.3237 - accuracy: 0.9562 - val_loss: 0.3870 - val_accuracy: 0.9518\nEpoch 8/20\n146/146 [==============================] - 7s 46ms/step - loss: 0.3117 - accuracy: 0.9558 - val_loss: 0.4322 - val_accuracy: 0.9553\nEpoch 9/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.3841 - accuracy: 0.9525 - val_loss: 0.4759 - val_accuracy: 0.9458\nEpoch 10/20\n146/146 [==============================] - 6s 42ms/step - loss: 0.3867 - accuracy: 0.9524 - val_loss: 0.4956 - val_accuracy: 0.9536\nEpoch 11/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.4344 - accuracy: 0.9569 - val_loss: 0.5131 - val_accuracy: 0.9514\nEpoch 12/20\n146/146 [==============================] - 6s 42ms/step - loss: 0.4486 - accuracy: 0.9560 - val_loss: 0.5210 - val_accuracy: 0.9493\nEpoch 13/20\n146/146 [==============================] - 6s 42ms/step - loss: 0.4467 - accuracy: 0.9546 - val_loss: 0.4882 - val_accuracy: 0.9471\nEpoch 14/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.4211 - accuracy: 0.9472 - val_loss: 0.4840 - val_accuracy: 0.9463\nEpoch 15/20\n146/146 [==============================] - 6s 42ms/step - loss: 0.4233 - accuracy: 0.9481 - val_loss: 0.4769 - val_accuracy: 0.9471\nEpoch 16/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.4114 - accuracy: 0.9470 - val_loss: 0.4656 - val_accuracy: 0.9458\nEpoch 17/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.4184 - accuracy: 0.9533 - val_loss: 0.4867 - val_accuracy: 0.9501\nEpoch 18/20\n146/146 [==============================] - 7s 46ms/step - loss: 0.4150 - accuracy: 0.9545 - val_loss: 0.4799 - val_accuracy: 0.9501\nEpoch 19/20\n146/146 [==============================] - 7s 48ms/step - loss: 0.4163 - accuracy: 0.9538 - val_loss: 0.4944 - val_accuracy: 0.9531\nEpoch 20/20\n146/146 [==============================] - 7s 46ms/step - loss: 0.4306 - accuracy: 0.9569 - val_loss: 0.4911 - val_accuracy: 0.9518\n\n\nException ignored in: &lt;function _xla_gc_callback at 0x7a26d6e763b0&gt;\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/lib/__init__.py\", line 97, in _xla_gc_callback\n    def _xla_gc_callback(*args):\nKeyboardInterrupt: \n\n\n\nimport matplotlib.pyplot as plt\n\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.ylim([0, 1])\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\nplt.show()\n\n\n\n\n\n\n\n\nValidation Accuracy: The accuracy of my model4 stabilized between 94% and 97% during training.\nComparison to model1: Compare to the model1’s 63% accuracy, there is an around 45% improvement.\nOverfitting Observation: we don’t observe much overfitting in model 4.\n6. Score on Test Data\nNow let’s evaluate the accuracy of our most performant model on the unseen test_ds: We choose Model 4!\n\nresults = model4.evaluate(test_ds) # Evaluate the model on the test dataset\nprint(f'Test accuracy is {results[1]}') # f-string allows embeded expressions inside string literals for formatting\nprint(f'Test lost is {results[0]}') # inside the braces, expressions are evaluated\n\n37/37 [==============================] - 1s 38ms/step - loss: 0.4854 - accuracy: 0.9570\nTest accuracy is 0.9570077657699585\nTest lost is 0.48544150590896606\n\n\nThe entire f-string generates a string that includes both the static text and the evaluated expression, which is then passed to the print function to be displayed.\nHere is a recap about f-string if you need: f-string documentation1; f-string documentation2\nIn this way, we evaluate the accuracy of our most performant model4 on the unseen test_ds,\nits latest test accuracy is approximately 95.70%.\nAt the end, just for reference, we also present accuracy of model1, model2, model3.\n\nresults = model1.evaluate(test_ds)\nprint(f'Test accuracy is {results[1]}') #\nprint(f'Test lost is {results[0]}')\n\n37/37 [==============================] - 0s 12ms/step - loss: 1.7082 - accuracy: 0.6341\nTest accuracy is 0.6341358423233032\nTest lost is 1.7081671953201294\n\n\n\nresults = model2.evaluate(test_ds)\nprint(f'Test accuracy is {results[1]}') #\nprint(f'Test lost is {results[0]}')\n\n37/37 [==============================] - 0s 12ms/step - loss: 0.5341 - accuracy: 0.7360\nTest accuracy is 0.7360275387763977\nTest lost is 0.5340545773506165\n\n\n\nresults = model3.evaluate(test_ds)\nprint(f'Test accuracy is {results[1]}') #\nprint(f'Test lost is {results[0]}')\n\n37/37 [==============================] - 1s 13ms/step - loss: 0.4324 - accuracy: 0.8332\nTest accuracy is 0.8331900238990784\nTest lost is 0.4323917031288147\n\n\nThank you for your time reading this tutorial."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/16BHW6/index.html",
    "href": "posts/16BHW6/index.html",
    "title": "PIC16B HW6",
    "section": "",
    "text": "The rapid spread of fake news, fueled by the internet and unvetted content sharing on digital platforms, has emerged as a significant global concern. In this case, analyzing fake news serves as a crucial defense mechanism. In today’s tutorial, we are going to learn how to develop and assess a fake news classifier using Keras.\nData Source: Our data for this tutorial is from this article\nAhmed H, Traore I, Saad S. (2017) “Detection of Online Fake News Using N-Gram Analysis and Machine Learning Techniques. In: Traore I., Woungang I., Awad A. (eds) Intelligent, Secure, and Dependable Systems in Distributed and Cloud Environments. ISDDC 2017. Lecture Notes in Computer Science, vol 10618. Springer, Cham (pp. 127-138).\n\n\n\n\n\nThe dataset we plan to use hosted a training data set at the below URL. train_url = “https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true”.  We can read it into Python directly, or you can choose to download it to your computer\n\n#import libraries:\nimport pandas as pd\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\nimport nltk\nfrom nltk.corpus import stopwords\n\n# Download NLTK stopwords\nnltk.download('stopwords')\n# NLTK stopwords (we need english version)\nnltk_stopwords = set(stopwords.words('english'))\n\n# Acquire Training Data\ndef acquire_training_data(url):\n    data = pd.read_csv(url)\n    return data\ntrain_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true\"\n# Acquire the training data\ntraining_data = acquire_training_data(train_url)\n#training_data_data = pd.read_csv(\"./fake_news_train.csv\")\nprint(training_data.head())\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/mabook/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\n   Unnamed: 0                                              title  \\\n0       17366  Merkel: Strong result for Austria's FPO 'big c...   \n1        5634       Trump says Pence will lead voter fraud panel   \n2       17487  JUST IN: SUSPECTED LEAKER and “Close Confidant...   \n3       12217  Thyssenkrupp has offered help to Argentina ove...   \n4        5535  Trump say appeals court decision on travel ban...   \n\n                                                text  fake  \n0  German Chancellor Angela Merkel said on Monday...     0  \n1  WEST PALM BEACH, Fla.President Donald Trump sa...     0  \n2  On December 5, 2017, Circa s Sara Carter warne...     1  \n3  Germany s Thyssenkrupp, has offered assistance...     0  \n4  President Donald Trump on Thursday called the ...     0  \n\n\n\n\n\nWe now will write a function “make_dataset”, which - 1. Change the text to lowercase. - 2. Remove stopwords from the article text and title. - 3. Construct and return a tf.data.Dataset with two inputs and one output. The input should be of the form (title,text) ,and the output should consist only of the fake column. \nSince batch can greatly increase the speeding of traning, we will also need to batch our dataset prior to returning it using “my_data_set.batch(100)”.\n\nimport tensorflow as tf\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\n\n# Download NLTK stopwords\nnltk.download('stopwords')\n\n# Function to create the dataset\n# makde_dataset is implemented as a function, and used to create both the training/validation and testing data sets\ndef make_dataset(data):\n    # Function to preprocess text\n    def preprocess_text(text):\n        # Convert text to lowercase\n        text = text.lower()\n        # Remove punctuation\n        text = text.translate(str.maketrans('', '', string.punctuation))\n        # Remove stopwords using NLTK\n        stopwords_nltk = set(stopwords.words('english'))\n        words = text.split()\n        text = ' '.join([word for word in words if word not in stopwords_nltk])\n        \n        # Remove dash \"–\" this step is not required but necessary later.\n        # We want the most frequent words to be meaningful and indicative\n        # So we need to remove \"-\"\n        text = text.replace('–', '')\n        return text\n\n    # Apply preprocessing to title and text columns\n    data['title'] = data['title'].apply(preprocess_text)\n    data['text'] = data['text'].apply(preprocess_text)\n    \n    # Create tf.data.Dataset\n    dataset = tf.data.Dataset.from_tensor_slices((\n        {\"title\": data['title'], \"text\": data['text']},  # Inputs\n        data['fake']  # Output\n    ))\n    #Here we make sure that the constructed Dataset has multiple inputs\n    \n    # Batch the dataset\n    dataset = dataset.batch(100)\n    \n    return dataset\n\n# Example usage:\n# Assuming train_data is your training DataFrame loaded from the CSV file\ntrain_dataset = make_dataset(training_data)\ntrain_data=train_dataset\n# Example of iterating through the dataset\nfor inputs, output in train_dataset.take(1):\n    print(\"Title:\", inputs['title'][0].numpy())\n    print(\"Text:\", inputs['text'][0].numpy())\n    print(\"Fake:\", output[0].numpy())\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/mabook/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n2024-03-11 23:17:42.273937: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_2' with dtype int64 and shape [22449]\n     [[{{node Placeholder/_2}}]]\n\n\nTitle: b'merkel strong result austrias fpo big challenge parties'\nText: b'german chancellor angela merkel said monday strong showing austria antiimmigrant freedom party fpo sunday election big challenge parties speaking news conference berlin merkel added hoping close cooperation austria conservative election winner sebastian kurz european level'\nFake: 0\n\n\n\n\nNow we have constructed our primary Dataset. We split 20% of it to use for validation. Then determining a base rate is also an important step. We will determine the base rate for this data set by examining the labels on the training set.\n\ndef split_train_val_dataset(dataset, val_ratio=0.2): #the ratio fo validation set, we set 0.2 (20%)\n    # Determine sizes of train, val\n    val_size = int(val_ratio * len(dataset))\n    \n    # Split dataset into train, validation\n    train_dataset = dataset.skip(val_size)\n    val_dataset = dataset.take(val_size)\n    \n    return train_dataset, val_dataset\n\n# Split the training and validation data sets using a function\ntrain_data, val_data= split_train_val_dataset(train_data, val_ratio=0.2)\ntrain=train_data\nval=val_data\ntest_url=\"https://raw.githubusercontent.com/PhilChodrow/PIC16B/master/datasets/fake_news_test.csv\"\ntest_dataset = acquire_training_data(test_url)\n\ntest_data = make_dataset(test_dataset)\n\n# Step 5: Calculate base rate\ndef calculate_base_rate(data):\n    # Count the number of fake news and real news articles\n    num_fake = sum(data['fake'])\n    num_real = len(data) - num_fake\n\n    # Calculate the proportion of fake news articles in the dataset\n    fake_proportion = num_fake / len(data)\n    \n    # For the Base Rate for the data set：\n    # The base rate represents the proportion of fake news articles in the dataset.\n    # It provides a baseline for evaluating the performance of the fake news detection model.\n    # A higher base rate indicates a larger proportion of fake news articles in the dataset.\n    # The model's performance should be assessed relative to this base rate.\n    \n    base_rate = fake_proportion\n\n    return base_rate, num_fake, num_real\n\nbase_rate, num_fake, num_real = calculate_base_rate(training_data)\n\n\nprint(\"Base rate:\", base_rate)\nprint(\"Number of fake news articles:\", num_fake)\nprint(\"Number of real news articles:\", num_real)\n\nBase rate: 0.522963160942581\nNumber of fake news articles: 11740\nNumber of real news articles: 10709\n\n\nThe base rate of 0.5229 indicates that approximately 52.3% of the articles in the dataset are labeled as fake news. This means that if we were to randomly guess the label of an article without using any model or additional information, we would have a 52.3% chance of correctly identifying it as fake news.\nIn the next step, we import the re module for regular expressions: we define the vocabulary size as 2000. Then, we create a standardization function to preprocess the text by converting it to lowercase and removing punctuation using regular expressions. We can then create a TextVectorization layer called title_vectorize_layer. We configure it with the standardization function, vocabulary size, output mode as integers, and output sequence length of 500.\nFinally, we adapt the title_vectorize_layer to the training data by extracting the “title” field from the dataset using train_data.map(lambda x, y: x[“title”]). This process creates a TextVectorization layer that can transform text titles into fixed-length integer sequences, ready to be used in a neural network model.\n\nimport re\n# Define the size of the vocabulary\nsize_vocabulary = 2000\n\n# Define the standardization function\ndef standardization(input_data):\n    # Convert text to lowercase\n    lowercase = tf.strings.lower(input_data)\n    # Remove punctuation using regular expression\n    no_punctuation = tf.strings.regex_replace(lowercase, '[%s]' % re.escape(string.punctuation), '')\n    return no_punctuation \n\n# Create a TextVectorization layer\ntitle_vectorize_layer = tf.keras.layers.TextVectorization(\n    standardize=standardization,\n    max_tokens=size_vocabulary,  # only consider this many words\n    output_mode='int',\n    output_sequence_length=500\n)\n\n# Adapt the TextVectorization layer to the training data\ntitle_vectorize_layer.adapt(train_data.map(lambda x, y: x[\"title\"]))\n\n2024-03-11 23:18:13.471149: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_2' with dtype int64 and shape [22449]\n     [[{{node Placeholder/_2}}]]\n\n\n\n#As for Text Vectoriazation, there is one option of code:\n\"\"\"\npreparing a text vectorization layer for tf model\nsize_vocabulary = 2000\n\ndef standardization(input_data):\n    lowercase = tf.strings.lower(input_data)\n    no_punctuation = tf.strings.regex_replace(lowercase,\n                                  '[%s]' % re.escape(string.punctuation),'')\n    return no_punctuation \n\ntitle_vectorize_layer = TextVectorization(\n    standardize=standardization,\n    max_tokens=size_vocabulary, # only consider this many words\n    output_mode='int',\n    output_sequence_length=500) \n\ntitle_vectorize_layer.adapt(train.map(lambda x, y: x[\"title\"]))\"\"\"\n\n'\\npreparing a text vectorization layer for tf model\\nsize_vocabulary = 2000\\n\\ndef standardization(input_data):\\n    lowercase = tf.strings.lower(input_data)\\n    no_punctuation = tf.strings.regex_replace(lowercase,\\n                                  \\'[%s]\\' % re.escape(string.punctuation),\\'\\')\\n    return no_punctuation \\n\\ntitle_vectorize_layer = TextVectorization(\\n    standardize=standardization,\\n    max_tokens=size_vocabulary, # only consider this many words\\n    output_mode=\\'int\\',\\n    output_sequence_length=500) \\n\\ntitle_vectorize_layer.adapt(train.map(lambda x, y: x[\"title\"]))'\n\n\n\n\n\n\nNow we are ready to use Keras models to offer a perspective on the following question:  “When detecting fake news, is it most effective to focus on only the title of the article, the full text of the article, or both?”  To offer effective analysis, we need to create following three models:\n\n\n\nModel\nInput\n\n\n\n\nModel 1\nOnly the article title as an input\n\n\nModel 2\nOnly the article text as an input\n\n\nModel 3\nBoth the article title and the article text as input\n\n\n\n\n\n\n#import libraries\nimport re\nimport string\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\nfrom tensorflow.keras import losses\nimport matplotlib.pyplot as plt\n\n\"\"\"Now we adopt TextVectorization introduced before here\"\"\"\n# Define size of vocabulary\nsize_vocabulary = 2000\n\n# Define standardization function for preprocessing\ndef standardization(input_data):\n    lowercase = tf.strings.lower(input_data)\n    no_punctuation = tf.strings.regex_replace(lowercase,\n                                  '[%s]' % re.escape(string.punctuation),'')\n    return no_punctuation \n\n# Define text vectorization layer for title\ntitle_vectorize_layer = TextVectorization(standardize=standardization,\n    max_tokens=size_vocabulary,\n    output_mode='int',\n    output_sequence_length=500) \n\n# Adapt text vectorization layer to the training data for titles\ntitle_vectorize_layer.adapt(train.map(lambda x, y: x[\"title\"]))\n\n# Define text vectorization layer for text\ntext_vectorize_layer = TextVectorization(\n    standardize=standardization,\n    max_tokens=size_vocabulary,\n    output_mode='int',\n    output_sequence_length=500) \n\n2024-03-11 23:43:32.266746: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_2' with dtype int64 and shape [22449]\n     [[{{node Placeholder/_2}}]]\n\n\n\n# Adapt text vectorization layer to the training data for text\ntext_vectorize_layer.adapt(train.map(lambda x, y: x[\"text\"]))\n\n# Define inputs for title model (Model 1)\ntitles_input = Input(shape=(1,), name=\"title\", dtype=\"string\")\n\n# Apply the title_vectorize_layer to the titles_input\ntitle_features = title_vectorize_layer(titles_input) \n\n# Add an Embedding layer to learn dense vector representations for each word in the title\ntitle_features = layers.Embedding(size_vocabulary, output_dim=3, name=\"embedding_title\")(title_features)\n# Apply Dropout regularization to the embedded title features to reduce overfitting\ntitle_features = layers.Dropout(0.2)(title_features)\n\n# Reduce the spatial dimensions and obtain a fixed-length vector\ntitle_features = layers.GlobalAveragePooling1D()(title_features)\n# Apply another Dropout regularization to the pooled title features\ntitle_features = layers.Dropout(0.2)(title_features)\n\n# Add a Dense layer with 32 units and ReLU activation for further feature transformation\ntitle_features = layers.Dense(32, activation='relu')(title_features)\n# Add the final output layer with 2 units (assuming binary classification)\noutput = layers.Dense(2, name=\"fake\")(title_features)\n\n2024-03-11 23:43:34.018390: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_2' with dtype int64 and shape [22449]\n     [[{{node Placeholder/_2}}]]\n\n\n\n\n\n\n# We define a visualization function here using minimal repetition rule:\ndef plot_training_history(history, model_name):\n    plt.plot(history.history[\"accuracy\"], label=\"training\")  # Plot the training accuracy over epochs\n    plt.plot(history.history[\"val_accuracy\"], label=\"validation\")  # Plot the validation accuracy over epochs\n    plt.xlabel(\"Epoch\")  # Set the x-axis label to \"Epoch\"\n    plt.ylabel(\"Accuracy\")  # Set the y-axis label to \"Accuracy\"\n    plt.title(f'{model_name} Training History')  # Set the title of the plot based on the model name\n    plt.legend()  # Display the legend\n    plt.show()  # Render and display the plot\n\n\n# Define and compile title model\nmodel1 = Model(inputs=titles_input,outputs=output)\nmodel1.compile(optimizer=\"adam\", loss=losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n\n# Train title model\nhistory = model1.fit(train, validation_data=val,epochs=50, verbose=False)\n\n# Plot training history for title model\nplot_training_history(history, 'Model 1')\n\n# Define inputs for text model\ntext_input = Input(shape=(1,), name=\"text\",dtype=\"string\")\n\n# Apply text vectorization layer\ntext_features = text_vectorize_layer(text_input)\n# Add an Embedding layer to learn dense vector representations for each word in the text\ntext_features = layers.Embedding(size_vocabulary, output_dim=7, name=\"embedding_text\")(text_features)\n#Apply Dropout regularization to the embedded text features to reduce overfitting\ntext_features = layers.Dropout(0.2)(text_features)\n#reduce the spatial dimensions and obtain a fixed-length vector\ntext_features = layers.GlobalAveragePooling1D()(text_features)\ntext_features = layers.Dropout(0.2)(text_features)\n# Add a Dense layer with 32 units and ReLU activation for further feature transformation\ntext_features = layers.Dense(32, activation='relu')(text_features)\n# Add the final output layer with 2 units (binary classification)\noutput = layers.Dense(2, name=\"fake\")(text_features)\n\n\n\n\n\n\n\n\n\n\n\n\n# Define and compile text model (Model 2)\nmodel2 = Model(\n    inputs=text_input,\n    outputs=output\n)\nmodel2.compile(optimizer=\"adam\",  # Use the Adam optimizer for training\n              loss=losses.SparseCategoricalCrossentropy(from_logits=True), # Use Sparse Categorical Cross-entropy loss\n              metrics=[\"accuracy\"]) # Track accuracy as a metric during training\n\n# Train text model\nhistory = model2.fit(train,  # Use the training dataset\n                    validation_data=val, # Use the validation dataset for validation during training\n                    epochs=50,  # Train for 50 epochs\n                    verbose=False) # Disable verbose output during training\n\n# Plot training history for text model\nplot_training_history(history, 'Model 2')\n\n# Define inputs for combined model \ntitles_input = Input(shape=(1,), name=\"title\", dtype=\"string\")\ntext_input = Input(shape=(1,), name=\"text\", dtype=\"string\")\n\n# Apply the title_vectorize_layer to the titles_input\ntitle_features = title_vectorize_layer(titles_input)\ntext_features = text_vectorize_layer(text_input)\n\ntitle_features = layers.Embedding(size_vocabulary, output_dim=3, name=\"embedding_title\")(title_features)\ntext_features = layers.Embedding(size_vocabulary, output_dim=7, name=\"embedding_text\")(text_features)\n\n# Apply Dropout regularization to the embedded title features to prevent overfitting\ntitle_features = layers.Dropout(0.2)(title_features)\ntext_features = layers.Dropout(0.2)(text_features)\n\ntitle_features = layers.GlobalAveragePooling1D()(title_features)\ntext_features = layers.GlobalAveragePooling1D()(text_features)\n\ntitle_features = layers.Dense(32, activation='relu')(title_features)\ntext_features = layers.Dense(32, activation='relu')(text_features)\n\n# Concatenate title and text features\nmain = layers.concatenate([title_features, text_features], axis=1)\n# axis=1 means concatenation along the feature dimension\n# This combines the extracted features from both the title and text inputs\n\noutput = layers.Dense(2, name=\"fake\")(main)\n# The Dense layer has 2 units, corresponding to the two classes (real and fake)\n# The \"fake\" name is assigned to this output layer\n# The output layer takes the concatenated features (main) as input\n\n\n\n\n\n\n\n\n\n\n\n\n# Define and compile combined model (Model 3)\nmodel3 = Model(inputs=[titles_input, text_input], outputs=output) # The model's output is the output layer defined above\nmodel3.compile(optimizer=\"adam\",\n              loss=losses.SparseCategoricalCrossentropy(from_logits=True), \n              # Sparse Categorical Cross-entropy is used as the loss function since the labels are integers\n              metrics=[\"accuracy\"]) # Track accuracy as a metric during training\n\n# Train combined model\nhistory = model3.fit(train, validation_data=val,epochs=50, verbose=False)\n\n# Plot training history for combined model 3\nplot_training_history(history, 'Model 3')\n\n\n\n\n\n\n\n\n\n# Evaluate model1, model 2, model 3 on the validation dataset and training dataset\nmodels = [model1, model2, model3]\ndataset_names = [\"Validation\", \"Training\"]\ndatasets = [val, train]\n\nfor i, model in enumerate(models, start=1):\n    for dataset_name, dataset in zip(dataset_names, datasets):\n        loss, accuracy = model.evaluate(dataset)\n        print(f\"Model {i} {dataset_name} Accuracy:\", accuracy)\n\n 1/45 [..............................] - ETA: 0s - loss: 0.2773 - accuracy: 0.910045/45 [==============================] - 0s 2ms/step - loss: 0.1387 - accuracy: 0.9467\nModel 1 Validation Accuracy: 0.9466666579246521\n180/180 [==============================] - 0s 1ms/step - loss: 0.0890 - accuracy: 0.9655\nModel 1 Training Accuracy: 0.9654576778411865\n45/45 [==============================] - 0s 4ms/step - loss: 0.1737 - accuracy: 0.9762\nModel 2 Validation Accuracy: 0.9762222170829773\n180/180 [==============================] - 1s 4ms/step - loss: 0.0078 - accuracy: 0.9989\nModel 2 Training Accuracy: 0.9988857507705688\n45/45 [==============================] - 0s 5ms/step - loss: 0.0880 - accuracy: 0.9849\nModel 3 Validation Accuracy: 0.9848889112472534\n180/180 [==============================] - 1s 4ms/step - loss: 7.5198e-04 - accuracy: 0.9998\nModel 3 Training Accuracy: 0.9997771382331848\n\n\n\n\n\nModel\nTraining Accuracy\nValidation Accuracy\n\n\n\n\nModel 1\n0.9655\n0.9467\n\n\nModel 2\n0.9989\n0.9762\n\n\nModel 3\n0.9998\n0.9849\n\n\n\nOverall, we observe that model 3 demonstrates the best performance among the three models, with the highest training and validation accuracies. Model 2 also performs well but shows a slightly gap between training and validation accuracies compared to Model 3. Model 1, while still achieving good accuracies, has the lowest performance among the three models.\n\n\n\n\n\nNow, we will test our model 3 performance on unseen test data\nWe can download the test data here : “test_url =”https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true”“. Previously, in data preparation section (Part 2), we already convert this data using the make_dataset function we defined. Below is a recap:\n# Split the training and validation data sets using a function\ntrain_data, val_data = split_train_val_dataset(train_data, val_ratio=0.2)\ntrain = train_data\nval = val_data\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/PIC16B/master/datasets/fake_news_test.csv\"\ntest_dataset = acquire_training_data(test_url)\n\ntest_data = make_dataset(test_dataset)\nNow we evaluate model 3 on the data:\n\n# Evaluate Model 3\nevaluation_result = model3.evaluate(test_data)\n\n# Print evaluation results\nprint(\"Test Loss:\", evaluation_result[0])\nprint(\"Test Accuracy:\", evaluation_result[1])\n\n# Print visualization\nplt.bar([\"Loss\", \"Accuracy\"], evaluation_result, color=['blue', 'green'])\nplt.xlabel('Metrics')\nplt.ylabel('Values')\nplt.title('Model3 Evaluation Results on Test Data')\nplt.show()\n\n 50/225 [=====&gt;........................] - ETA: 0s - loss: 0.0904 - accuracy: 0.9830225/225 [==============================] - 1s 3ms/step - loss: 0.0837 - accuracy: 0.9837\nTest Loss: 0.08365771919488907\nTest Accuracy: 0.9836964011192322\n\n\n2024-03-11 23:51:50.256145: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_2' with dtype int64 and shape [22449]\n     [[{{node Placeholder/_2}}]]\n\n\n\n\n\n\n\n\n\nBased on the provided test results, if we used the model as a fake news detector, we would be correct approximately 98% of the time. A test accuracy of 98% suggests that the model has learned to distinguish between real and fake news with high accuracy based on the given dataset.\n\n\n\nNow it could be fun to look at the embedding learned by our model. We need to comment on at least 5 words whose location in the embedding you find interpretable.\n\n\nPCA is a dimensionality reduction technique that aims to transform high-dimensional data into a lower-dimensional space while preserving the most important information. It identifies the principal components, which are the directions of maximum variance in the data.\nBy applying PCA to the learned word embeddings, we can reduce their dimensionality and visualize them in a lower-dimensional space, such as 2D or 3D. This allows us to understand the structure and relationships between the word embeddings in a more interpretable way.\n\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nfrom sklearn.decomposition import PCA\n\n# Assuming you have obtained the embedding weights and vocabulary\n# Replace 'weights' and 'vocab' with actual values\nweights = model1.get_layer('embedding_title').get_weights()[0]  # Example embedding weights\nvocab = title_vectorize_layer.get_vocabulary()  # Example vocabulary\n\n# Perform PCA for dimensionality reduction\npca = PCA(n_components=3)\nweights_pca = pca.fit_transform(weights)\n\n# Extract embeddings for each word\nembedding_dict = {word: embedding for word, embedding in zip(vocab, weights_pca)}\n\n# Sort words based on embedding values\nsorted_words = sorted(embedding_dict, key=lambda x: np.linalg.norm(embedding_dict[x]), reverse=True)\n\n# Print the top words with highest embeddings\nnum_top_words = 5\nprint(f\"Top {num_top_words} words with highest embeddings:\")\nfor word in sorted_words[:num_top_words]:\n    print(f\"Word: {word}, Embedding: {embedding_dict[word]}\")\n\n# Perform additional analysis and commentary on the embeddings\n\n# Create DataFrame for plotting\nembedding_df = pd.DataFrame({\n    'word': vocab,\n    'x0': weights_pca[:, 0],\n    'x1': weights_pca[:, 1],\n    'x2': weights_pca[:, 2]\n})\n\n# Set the renderer to \"iframe\"\nimport plotly.io as pio\npio.renderers.default = \"iframe\"\n\n# Create a scatter plot using Plotly Express\nfig = px.scatter_3d(embedding_df, x='x0', y='x1', z='x2', hover_name='word')\n# Each point represents a word, positioned based on its 'x0', 'x1', and 'x2' coordinates\n# Hovering over a point will show the corresponding word\n\n# Save the plot as an HTML file\noutput_path = \"scatter_plot.html\"\nfig.write_html(output_path)\n\n# Display the plot\nfig.show()\n\nTop 5 words with highest embeddings:\nWord: video, Embedding: [ 5.4536767  -0.08328094  0.03338596]\nWord: factbox, Embedding: [-5.2003880e+00 -1.1863625e-03 -3.2642100e-02]\nWord: trump’s, Embedding: [ 4.952767    0.07439824 -0.00521921]\nWord: obama’s, Embedding: [ 4.718474    0.04242248 -0.06171212]\nWord: gop, Embedding: [ 4.650868   -0.03748338 -0.00787986]\n\n\n\n\n\nPCA Embedding:\n\n“video”: This word has the highest embedding in the PCA plot, with coordinates [5.666067, 0.06447994, 0.02214868]. Its location indicates that it is relatively distinct from other words in the embedding space.\n“factbox”: The word “factbox” has the second-highest embedding, with coordinates [-5.255509, -0.02237466, 0.04613917]. Its negative x-coordinate suggests that it is located on the opposite side of the embedding space compared to the other top words.\n“trump’s”: The word “trump’s” has the third-highest embedding, with coordinates [5.005154, 0.03613936, 0.02672017]. Its proximity to “video” indicates that these words may share some semantic similarities or appear in similar contexts.\n“obama’s”: The word “obama’s” has the fourth-highest embedding, with coordinates [4.778542, 0.02571037, -0.04872589]. Its location near “trump’s” suggests that it is semantically related to this word.\n“gop”: The word “gop” has the fifth-highest embedding, with coordinates [4.7234287, 0.00562899, -0.02563044]. Its proximity to “trump’s” and “obama’s” indicates that it may share some contextual similarities with these words.\n\n\n\n\nT-SNE is a widely used technique for visualizing high-dimensional data in a lower-dimensional space, such as 2D or 3D. It aims to preserve the local structure of the data while revealing global patterns and clusters. By visualizing the word embeddings using t-SNE, we can gain insights into the semantic structure and relationships between words. Words that are semantically similar or related are expected to be clustered together in the t-SNE plot.\n\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nfrom sklearn.manifold import TSNE\n\n# Assuming you have obtained the embedding weights and vocabulary\n# Replace 'weights' and 'vocab' with actual values\nweights = model1.get_layer('embedding_title').get_weights()[0]  # Example embedding weights\nvocab = title_vectorize_layer.get_vocabulary()  # Example vocabulary\n\n# Perform t-SNE for dimensionality reduction\ntsne = TSNE(n_components=3)\nweights_tsne = tsne.fit_transform(weights)\n\n# Extract embeddings for each word\nembedding_dict = {word: embedding for word, embedding in zip(vocab, weights_tsne)}\n\n# Sort words based on embedding values\nsorted_words = sorted(embedding_dict, key=lambda x: np.linalg.norm(embedding_dict[x]), reverse=True)\n\n# Print the top words with highest embeddings\nnum_top_words = 5\nprint(f\"Top {num_top_words} words with highest embeddings:\")\nfor word in sorted_words[:num_top_words]:\n    print(f\"Word: {word}, Embedding: {embedding_dict[word]}\")\n\n# Create DataFrame for plotting\nembedding_df = pd.DataFrame({\n    'word': vocab,\n    'x0': weights_tsne[:, 0],\n    'x1': weights_tsne[:, 1],\n    'x2': weights_tsne[:, 2]\n})\n\n# Set the renderer to \"iframe\"\nimport plotly.io as pio\npio.renderers.default = \"iframe\"\n\n# Create a scatter plot using Plotly Express\nfig = px.scatter_3d(embedding_df, x='x0', y='x1', z='x2', hover_name='word')\n# Each point represents a word, positioned based on its 'x0', 'x1', and 'x2' coordinates\n# Hovering over a point will show the corresponding word\n\n# Save the plot as an HTML file\noutput_path = \"tsne_scatter_plot.html\"\nfig.write_html(output_path)\n\n# Display the plot\nfig.show()\n\nTop 5 words with highest embeddings:\nWord: factbox, Embedding: [-20.716244    1.6520227  11.792851 ]\nWord: rohingya, Embedding: [-20.60748     1.8122349  11.386174 ]\nWord: trumps, Embedding: [-20.589226   1.833724  11.317928]\nWord: macron, Embedding: [-20.500938    1.8953061  11.031187 ]\nWord: catalan, Embedding: [-20.492176    1.8876332  11.009249 ]\n\n\n\n\n\nt-SNE Embedding:\n\n“factbox”: This word has the highest embedding in the t-SNE plot, with coordinates [-20.716244, 1.6520227, 11.792851]. Its location suggests that it is relatively distinct from other words in the embedding space.\n“rohingya”: The word “rohingya” has the second-highest embedding, with coordinates [-20.60748, 1.8122349, 11.386174]. Its proximity to “factbox” indicates that these words may share some semantic similarities or appear in similar contexts.\n“trumps”: The word “trumps” has the third-highest embedding, with coordinates [-20.589226, 1.833724, 11.317928]. Its location near “rohingya” and “factbox” suggests that it is also semantically related to these words.\n“macron”: The word “macron” has the fourth-highest embedding, with coordinates [-20.500938, 1.8953061, 11.031187]. Its proximity to the previous words indicates that it may share some contextual similarities with them.\n“catalan”: The word “catalan” has the fifth-highest embedding, with coordinates [-20.492176, 1.8876332, 11.009249]. Its location close to the other top words suggests that it is semantically related to them.\n\n\n\n\nNow we finish the presenting of the learned word embedding and we show the written text discusses at least 5 words whose location is interpretable within the embedding. Thank you for your time."
  },
  {
    "objectID": "posts/16BHW6/index.html#introduction",
    "href": "posts/16BHW6/index.html#introduction",
    "title": "PIC16B HW6",
    "section": "",
    "text": "The rapid spread of fake news, fueled by the internet and unvetted content sharing on digital platforms, has emerged as a significant global concern. In this case, analyzing fake news serves as a crucial defense mechanism. In today’s tutorial, we are going to learn how to develop and assess a fake news classifier using Keras.\nData Source: Our data for this tutorial is from this article\nAhmed H, Traore I, Saad S. (2017) “Detection of Online Fake News Using N-Gram Analysis and Machine Learning Techniques. In: Traore I., Woungang I., Awad A. (eds) Intelligent, Secure, and Dependable Systems in Distributed and Cloud Environments. ISDDC 2017. Lecture Notes in Computer Science, vol 10618. Springer, Cham (pp. 127-138)."
  },
  {
    "objectID": "posts/16BHW6/index.html#instruction",
    "href": "posts/16BHW6/index.html#instruction",
    "title": "PIC16B HW6",
    "section": "",
    "text": "The dataset we plan to use hosted a training data set at the below URL. train_url = “https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true”.  We can read it into Python directly, or you can choose to download it to your computer\n\n#import libraries:\nimport pandas as pd\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\nimport nltk\nfrom nltk.corpus import stopwords\n\n# Download NLTK stopwords\nnltk.download('stopwords')\n# NLTK stopwords (we need english version)\nnltk_stopwords = set(stopwords.words('english'))\n\n# Acquire Training Data\ndef acquire_training_data(url):\n    data = pd.read_csv(url)\n    return data\ntrain_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true\"\n# Acquire the training data\ntraining_data = acquire_training_data(train_url)\n#training_data_data = pd.read_csv(\"./fake_news_train.csv\")\nprint(training_data.head())\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/mabook/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\n   Unnamed: 0                                              title  \\\n0       17366  Merkel: Strong result for Austria's FPO 'big c...   \n1        5634       Trump says Pence will lead voter fraud panel   \n2       17487  JUST IN: SUSPECTED LEAKER and “Close Confidant...   \n3       12217  Thyssenkrupp has offered help to Argentina ove...   \n4        5535  Trump say appeals court decision on travel ban...   \n\n                                                text  fake  \n0  German Chancellor Angela Merkel said on Monday...     0  \n1  WEST PALM BEACH, Fla.President Donald Trump sa...     0  \n2  On December 5, 2017, Circa s Sara Carter warne...     1  \n3  Germany s Thyssenkrupp, has offered assistance...     0  \n4  President Donald Trump on Thursday called the ...     0  \n\n\n\n\n\nWe now will write a function “make_dataset”, which - 1. Change the text to lowercase. - 2. Remove stopwords from the article text and title. - 3. Construct and return a tf.data.Dataset with two inputs and one output. The input should be of the form (title,text) ,and the output should consist only of the fake column. \nSince batch can greatly increase the speeding of traning, we will also need to batch our dataset prior to returning it using “my_data_set.batch(100)”.\n\nimport tensorflow as tf\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\n\n# Download NLTK stopwords\nnltk.download('stopwords')\n\n# Function to create the dataset\n# makde_dataset is implemented as a function, and used to create both the training/validation and testing data sets\ndef make_dataset(data):\n    # Function to preprocess text\n    def preprocess_text(text):\n        # Convert text to lowercase\n        text = text.lower()\n        # Remove punctuation\n        text = text.translate(str.maketrans('', '', string.punctuation))\n        # Remove stopwords using NLTK\n        stopwords_nltk = set(stopwords.words('english'))\n        words = text.split()\n        text = ' '.join([word for word in words if word not in stopwords_nltk])\n        \n        # Remove dash \"–\" this step is not required but necessary later.\n        # We want the most frequent words to be meaningful and indicative\n        # So we need to remove \"-\"\n        text = text.replace('–', '')\n        return text\n\n    # Apply preprocessing to title and text columns\n    data['title'] = data['title'].apply(preprocess_text)\n    data['text'] = data['text'].apply(preprocess_text)\n    \n    # Create tf.data.Dataset\n    dataset = tf.data.Dataset.from_tensor_slices((\n        {\"title\": data['title'], \"text\": data['text']},  # Inputs\n        data['fake']  # Output\n    ))\n    #Here we make sure that the constructed Dataset has multiple inputs\n    \n    # Batch the dataset\n    dataset = dataset.batch(100)\n    \n    return dataset\n\n# Example usage:\n# Assuming train_data is your training DataFrame loaded from the CSV file\ntrain_dataset = make_dataset(training_data)\ntrain_data=train_dataset\n# Example of iterating through the dataset\nfor inputs, output in train_dataset.take(1):\n    print(\"Title:\", inputs['title'][0].numpy())\n    print(\"Text:\", inputs['text'][0].numpy())\n    print(\"Fake:\", output[0].numpy())\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/mabook/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n2024-03-11 23:17:42.273937: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_2' with dtype int64 and shape [22449]\n     [[{{node Placeholder/_2}}]]\n\n\nTitle: b'merkel strong result austrias fpo big challenge parties'\nText: b'german chancellor angela merkel said monday strong showing austria antiimmigrant freedom party fpo sunday election big challenge parties speaking news conference berlin merkel added hoping close cooperation austria conservative election winner sebastian kurz european level'\nFake: 0\n\n\n\n\nNow we have constructed our primary Dataset. We split 20% of it to use for validation. Then determining a base rate is also an important step. We will determine the base rate for this data set by examining the labels on the training set.\n\ndef split_train_val_dataset(dataset, val_ratio=0.2): #the ratio fo validation set, we set 0.2 (20%)\n    # Determine sizes of train, val\n    val_size = int(val_ratio * len(dataset))\n    \n    # Split dataset into train, validation\n    train_dataset = dataset.skip(val_size)\n    val_dataset = dataset.take(val_size)\n    \n    return train_dataset, val_dataset\n\n# Split the training and validation data sets using a function\ntrain_data, val_data= split_train_val_dataset(train_data, val_ratio=0.2)\ntrain=train_data\nval=val_data\ntest_url=\"https://raw.githubusercontent.com/PhilChodrow/PIC16B/master/datasets/fake_news_test.csv\"\ntest_dataset = acquire_training_data(test_url)\n\ntest_data = make_dataset(test_dataset)\n\n# Step 5: Calculate base rate\ndef calculate_base_rate(data):\n    # Count the number of fake news and real news articles\n    num_fake = sum(data['fake'])\n    num_real = len(data) - num_fake\n\n    # Calculate the proportion of fake news articles in the dataset\n    fake_proportion = num_fake / len(data)\n    \n    # For the Base Rate for the data set：\n    # The base rate represents the proportion of fake news articles in the dataset.\n    # It provides a baseline for evaluating the performance of the fake news detection model.\n    # A higher base rate indicates a larger proportion of fake news articles in the dataset.\n    # The model's performance should be assessed relative to this base rate.\n    \n    base_rate = fake_proportion\n\n    return base_rate, num_fake, num_real\n\nbase_rate, num_fake, num_real = calculate_base_rate(training_data)\n\n\nprint(\"Base rate:\", base_rate)\nprint(\"Number of fake news articles:\", num_fake)\nprint(\"Number of real news articles:\", num_real)\n\nBase rate: 0.522963160942581\nNumber of fake news articles: 11740\nNumber of real news articles: 10709\n\n\nThe base rate of 0.5229 indicates that approximately 52.3% of the articles in the dataset are labeled as fake news. This means that if we were to randomly guess the label of an article without using any model or additional information, we would have a 52.3% chance of correctly identifying it as fake news.\nIn the next step, we import the re module for regular expressions: we define the vocabulary size as 2000. Then, we create a standardization function to preprocess the text by converting it to lowercase and removing punctuation using regular expressions. We can then create a TextVectorization layer called title_vectorize_layer. We configure it with the standardization function, vocabulary size, output mode as integers, and output sequence length of 500.\nFinally, we adapt the title_vectorize_layer to the training data by extracting the “title” field from the dataset using train_data.map(lambda x, y: x[“title”]). This process creates a TextVectorization layer that can transform text titles into fixed-length integer sequences, ready to be used in a neural network model.\n\nimport re\n# Define the size of the vocabulary\nsize_vocabulary = 2000\n\n# Define the standardization function\ndef standardization(input_data):\n    # Convert text to lowercase\n    lowercase = tf.strings.lower(input_data)\n    # Remove punctuation using regular expression\n    no_punctuation = tf.strings.regex_replace(lowercase, '[%s]' % re.escape(string.punctuation), '')\n    return no_punctuation \n\n# Create a TextVectorization layer\ntitle_vectorize_layer = tf.keras.layers.TextVectorization(\n    standardize=standardization,\n    max_tokens=size_vocabulary,  # only consider this many words\n    output_mode='int',\n    output_sequence_length=500\n)\n\n# Adapt the TextVectorization layer to the training data\ntitle_vectorize_layer.adapt(train_data.map(lambda x, y: x[\"title\"]))\n\n2024-03-11 23:18:13.471149: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_2' with dtype int64 and shape [22449]\n     [[{{node Placeholder/_2}}]]\n\n\n\n#As for Text Vectoriazation, there is one option of code:\n\"\"\"\npreparing a text vectorization layer for tf model\nsize_vocabulary = 2000\n\ndef standardization(input_data):\n    lowercase = tf.strings.lower(input_data)\n    no_punctuation = tf.strings.regex_replace(lowercase,\n                                  '[%s]' % re.escape(string.punctuation),'')\n    return no_punctuation \n\ntitle_vectorize_layer = TextVectorization(\n    standardize=standardization,\n    max_tokens=size_vocabulary, # only consider this many words\n    output_mode='int',\n    output_sequence_length=500) \n\ntitle_vectorize_layer.adapt(train.map(lambda x, y: x[\"title\"]))\"\"\"\n\n'\\npreparing a text vectorization layer for tf model\\nsize_vocabulary = 2000\\n\\ndef standardization(input_data):\\n    lowercase = tf.strings.lower(input_data)\\n    no_punctuation = tf.strings.regex_replace(lowercase,\\n                                  \\'[%s]\\' % re.escape(string.punctuation),\\'\\')\\n    return no_punctuation \\n\\ntitle_vectorize_layer = TextVectorization(\\n    standardize=standardization,\\n    max_tokens=size_vocabulary, # only consider this many words\\n    output_mode=\\'int\\',\\n    output_sequence_length=500) \\n\\ntitle_vectorize_layer.adapt(train.map(lambda x, y: x[\"title\"]))'\n\n\n\n\n\n\nNow we are ready to use Keras models to offer a perspective on the following question:  “When detecting fake news, is it most effective to focus on only the title of the article, the full text of the article, or both?”  To offer effective analysis, we need to create following three models:\n\n\n\nModel\nInput\n\n\n\n\nModel 1\nOnly the article title as an input\n\n\nModel 2\nOnly the article text as an input\n\n\nModel 3\nBoth the article title and the article text as input\n\n\n\n\n\n\n#import libraries\nimport re\nimport string\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\nfrom tensorflow.keras import losses\nimport matplotlib.pyplot as plt\n\n\"\"\"Now we adopt TextVectorization introduced before here\"\"\"\n# Define size of vocabulary\nsize_vocabulary = 2000\n\n# Define standardization function for preprocessing\ndef standardization(input_data):\n    lowercase = tf.strings.lower(input_data)\n    no_punctuation = tf.strings.regex_replace(lowercase,\n                                  '[%s]' % re.escape(string.punctuation),'')\n    return no_punctuation \n\n# Define text vectorization layer for title\ntitle_vectorize_layer = TextVectorization(standardize=standardization,\n    max_tokens=size_vocabulary,\n    output_mode='int',\n    output_sequence_length=500) \n\n# Adapt text vectorization layer to the training data for titles\ntitle_vectorize_layer.adapt(train.map(lambda x, y: x[\"title\"]))\n\n# Define text vectorization layer for text\ntext_vectorize_layer = TextVectorization(\n    standardize=standardization,\n    max_tokens=size_vocabulary,\n    output_mode='int',\n    output_sequence_length=500) \n\n2024-03-11 23:43:32.266746: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_2' with dtype int64 and shape [22449]\n     [[{{node Placeholder/_2}}]]\n\n\n\n# Adapt text vectorization layer to the training data for text\ntext_vectorize_layer.adapt(train.map(lambda x, y: x[\"text\"]))\n\n# Define inputs for title model (Model 1)\ntitles_input = Input(shape=(1,), name=\"title\", dtype=\"string\")\n\n# Apply the title_vectorize_layer to the titles_input\ntitle_features = title_vectorize_layer(titles_input) \n\n# Add an Embedding layer to learn dense vector representations for each word in the title\ntitle_features = layers.Embedding(size_vocabulary, output_dim=3, name=\"embedding_title\")(title_features)\n# Apply Dropout regularization to the embedded title features to reduce overfitting\ntitle_features = layers.Dropout(0.2)(title_features)\n\n# Reduce the spatial dimensions and obtain a fixed-length vector\ntitle_features = layers.GlobalAveragePooling1D()(title_features)\n# Apply another Dropout regularization to the pooled title features\ntitle_features = layers.Dropout(0.2)(title_features)\n\n# Add a Dense layer with 32 units and ReLU activation for further feature transformation\ntitle_features = layers.Dense(32, activation='relu')(title_features)\n# Add the final output layer with 2 units (assuming binary classification)\noutput = layers.Dense(2, name=\"fake\")(title_features)\n\n2024-03-11 23:43:34.018390: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_2' with dtype int64 and shape [22449]\n     [[{{node Placeholder/_2}}]]\n\n\n\n\n\n\n# We define a visualization function here using minimal repetition rule:\ndef plot_training_history(history, model_name):\n    plt.plot(history.history[\"accuracy\"], label=\"training\")  # Plot the training accuracy over epochs\n    plt.plot(history.history[\"val_accuracy\"], label=\"validation\")  # Plot the validation accuracy over epochs\n    plt.xlabel(\"Epoch\")  # Set the x-axis label to \"Epoch\"\n    plt.ylabel(\"Accuracy\")  # Set the y-axis label to \"Accuracy\"\n    plt.title(f'{model_name} Training History')  # Set the title of the plot based on the model name\n    plt.legend()  # Display the legend\n    plt.show()  # Render and display the plot\n\n\n# Define and compile title model\nmodel1 = Model(inputs=titles_input,outputs=output)\nmodel1.compile(optimizer=\"adam\", loss=losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n\n# Train title model\nhistory = model1.fit(train, validation_data=val,epochs=50, verbose=False)\n\n# Plot training history for title model\nplot_training_history(history, 'Model 1')\n\n# Define inputs for text model\ntext_input = Input(shape=(1,), name=\"text\",dtype=\"string\")\n\n# Apply text vectorization layer\ntext_features = text_vectorize_layer(text_input)\n# Add an Embedding layer to learn dense vector representations for each word in the text\ntext_features = layers.Embedding(size_vocabulary, output_dim=7, name=\"embedding_text\")(text_features)\n#Apply Dropout regularization to the embedded text features to reduce overfitting\ntext_features = layers.Dropout(0.2)(text_features)\n#reduce the spatial dimensions and obtain a fixed-length vector\ntext_features = layers.GlobalAveragePooling1D()(text_features)\ntext_features = layers.Dropout(0.2)(text_features)\n# Add a Dense layer with 32 units and ReLU activation for further feature transformation\ntext_features = layers.Dense(32, activation='relu')(text_features)\n# Add the final output layer with 2 units (binary classification)\noutput = layers.Dense(2, name=\"fake\")(text_features)\n\n\n\n\n\n\n\n\n\n\n\n\n# Define and compile text model (Model 2)\nmodel2 = Model(\n    inputs=text_input,\n    outputs=output\n)\nmodel2.compile(optimizer=\"adam\",  # Use the Adam optimizer for training\n              loss=losses.SparseCategoricalCrossentropy(from_logits=True), # Use Sparse Categorical Cross-entropy loss\n              metrics=[\"accuracy\"]) # Track accuracy as a metric during training\n\n# Train text model\nhistory = model2.fit(train,  # Use the training dataset\n                    validation_data=val, # Use the validation dataset for validation during training\n                    epochs=50,  # Train for 50 epochs\n                    verbose=False) # Disable verbose output during training\n\n# Plot training history for text model\nplot_training_history(history, 'Model 2')\n\n# Define inputs for combined model \ntitles_input = Input(shape=(1,), name=\"title\", dtype=\"string\")\ntext_input = Input(shape=(1,), name=\"text\", dtype=\"string\")\n\n# Apply the title_vectorize_layer to the titles_input\ntitle_features = title_vectorize_layer(titles_input)\ntext_features = text_vectorize_layer(text_input)\n\ntitle_features = layers.Embedding(size_vocabulary, output_dim=3, name=\"embedding_title\")(title_features)\ntext_features = layers.Embedding(size_vocabulary, output_dim=7, name=\"embedding_text\")(text_features)\n\n# Apply Dropout regularization to the embedded title features to prevent overfitting\ntitle_features = layers.Dropout(0.2)(title_features)\ntext_features = layers.Dropout(0.2)(text_features)\n\ntitle_features = layers.GlobalAveragePooling1D()(title_features)\ntext_features = layers.GlobalAveragePooling1D()(text_features)\n\ntitle_features = layers.Dense(32, activation='relu')(title_features)\ntext_features = layers.Dense(32, activation='relu')(text_features)\n\n# Concatenate title and text features\nmain = layers.concatenate([title_features, text_features], axis=1)\n# axis=1 means concatenation along the feature dimension\n# This combines the extracted features from both the title and text inputs\n\noutput = layers.Dense(2, name=\"fake\")(main)\n# The Dense layer has 2 units, corresponding to the two classes (real and fake)\n# The \"fake\" name is assigned to this output layer\n# The output layer takes the concatenated features (main) as input\n\n\n\n\n\n\n\n\n\n\n\n\n# Define and compile combined model (Model 3)\nmodel3 = Model(inputs=[titles_input, text_input], outputs=output) # The model's output is the output layer defined above\nmodel3.compile(optimizer=\"adam\",\n              loss=losses.SparseCategoricalCrossentropy(from_logits=True), \n              # Sparse Categorical Cross-entropy is used as the loss function since the labels are integers\n              metrics=[\"accuracy\"]) # Track accuracy as a metric during training\n\n# Train combined model\nhistory = model3.fit(train, validation_data=val,epochs=50, verbose=False)\n\n# Plot training history for combined model 3\nplot_training_history(history, 'Model 3')\n\n\n\n\n\n\n\n\n\n# Evaluate model1, model 2, model 3 on the validation dataset and training dataset\nmodels = [model1, model2, model3]\ndataset_names = [\"Validation\", \"Training\"]\ndatasets = [val, train]\n\nfor i, model in enumerate(models, start=1):\n    for dataset_name, dataset in zip(dataset_names, datasets):\n        loss, accuracy = model.evaluate(dataset)\n        print(f\"Model {i} {dataset_name} Accuracy:\", accuracy)\n\n 1/45 [..............................] - ETA: 0s - loss: 0.2773 - accuracy: 0.910045/45 [==============================] - 0s 2ms/step - loss: 0.1387 - accuracy: 0.9467\nModel 1 Validation Accuracy: 0.9466666579246521\n180/180 [==============================] - 0s 1ms/step - loss: 0.0890 - accuracy: 0.9655\nModel 1 Training Accuracy: 0.9654576778411865\n45/45 [==============================] - 0s 4ms/step - loss: 0.1737 - accuracy: 0.9762\nModel 2 Validation Accuracy: 0.9762222170829773\n180/180 [==============================] - 1s 4ms/step - loss: 0.0078 - accuracy: 0.9989\nModel 2 Training Accuracy: 0.9988857507705688\n45/45 [==============================] - 0s 5ms/step - loss: 0.0880 - accuracy: 0.9849\nModel 3 Validation Accuracy: 0.9848889112472534\n180/180 [==============================] - 1s 4ms/step - loss: 7.5198e-04 - accuracy: 0.9998\nModel 3 Training Accuracy: 0.9997771382331848\n\n\n\n\n\nModel\nTraining Accuracy\nValidation Accuracy\n\n\n\n\nModel 1\n0.9655\n0.9467\n\n\nModel 2\n0.9989\n0.9762\n\n\nModel 3\n0.9998\n0.9849\n\n\n\nOverall, we observe that model 3 demonstrates the best performance among the three models, with the highest training and validation accuracies. Model 2 also performs well but shows a slightly gap between training and validation accuracies compared to Model 3. Model 1, while still achieving good accuracies, has the lowest performance among the three models."
  },
  {
    "objectID": "posts/16BHW6/index.html#model-evaluation",
    "href": "posts/16BHW6/index.html#model-evaluation",
    "title": "PIC16B HW6",
    "section": "",
    "text": "Now, we will test our model 3 performance on unseen test data\nWe can download the test data here : “test_url =”https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true”“. Previously, in data preparation section (Part 2), we already convert this data using the make_dataset function we defined. Below is a recap:\n# Split the training and validation data sets using a function\ntrain_data, val_data = split_train_val_dataset(train_data, val_ratio=0.2)\ntrain = train_data\nval = val_data\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/PIC16B/master/datasets/fake_news_test.csv\"\ntest_dataset = acquire_training_data(test_url)\n\ntest_data = make_dataset(test_dataset)\nNow we evaluate model 3 on the data:\n\n# Evaluate Model 3\nevaluation_result = model3.evaluate(test_data)\n\n# Print evaluation results\nprint(\"Test Loss:\", evaluation_result[0])\nprint(\"Test Accuracy:\", evaluation_result[1])\n\n# Print visualization\nplt.bar([\"Loss\", \"Accuracy\"], evaluation_result, color=['blue', 'green'])\nplt.xlabel('Metrics')\nplt.ylabel('Values')\nplt.title('Model3 Evaluation Results on Test Data')\nplt.show()\n\n 50/225 [=====&gt;........................] - ETA: 0s - loss: 0.0904 - accuracy: 0.9830225/225 [==============================] - 1s 3ms/step - loss: 0.0837 - accuracy: 0.9837\nTest Loss: 0.08365771919488907\nTest Accuracy: 0.9836964011192322\n\n\n2024-03-11 23:51:50.256145: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_2' with dtype int64 and shape [22449]\n     [[{{node Placeholder/_2}}]]\n\n\n\n\n\n\n\n\n\nBased on the provided test results, if we used the model as a fake news detector, we would be correct approximately 98% of the time. A test accuracy of 98% suggests that the model has learned to distinguish between real and fake news with high accuracy based on the given dataset."
  },
  {
    "objectID": "posts/16BHW6/index.html#embedding-visualization",
    "href": "posts/16BHW6/index.html#embedding-visualization",
    "title": "PIC16B HW6",
    "section": "",
    "text": "Now it could be fun to look at the embedding learned by our model. We need to comment on at least 5 words whose location in the embedding you find interpretable.\n\n\nPCA is a dimensionality reduction technique that aims to transform high-dimensional data into a lower-dimensional space while preserving the most important information. It identifies the principal components, which are the directions of maximum variance in the data.\nBy applying PCA to the learned word embeddings, we can reduce their dimensionality and visualize them in a lower-dimensional space, such as 2D or 3D. This allows us to understand the structure and relationships between the word embeddings in a more interpretable way.\n\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nfrom sklearn.decomposition import PCA\n\n# Assuming you have obtained the embedding weights and vocabulary\n# Replace 'weights' and 'vocab' with actual values\nweights = model1.get_layer('embedding_title').get_weights()[0]  # Example embedding weights\nvocab = title_vectorize_layer.get_vocabulary()  # Example vocabulary\n\n# Perform PCA for dimensionality reduction\npca = PCA(n_components=3)\nweights_pca = pca.fit_transform(weights)\n\n# Extract embeddings for each word\nembedding_dict = {word: embedding for word, embedding in zip(vocab, weights_pca)}\n\n# Sort words based on embedding values\nsorted_words = sorted(embedding_dict, key=lambda x: np.linalg.norm(embedding_dict[x]), reverse=True)\n\n# Print the top words with highest embeddings\nnum_top_words = 5\nprint(f\"Top {num_top_words} words with highest embeddings:\")\nfor word in sorted_words[:num_top_words]:\n    print(f\"Word: {word}, Embedding: {embedding_dict[word]}\")\n\n# Perform additional analysis and commentary on the embeddings\n\n# Create DataFrame for plotting\nembedding_df = pd.DataFrame({\n    'word': vocab,\n    'x0': weights_pca[:, 0],\n    'x1': weights_pca[:, 1],\n    'x2': weights_pca[:, 2]\n})\n\n# Set the renderer to \"iframe\"\nimport plotly.io as pio\npio.renderers.default = \"iframe\"\n\n# Create a scatter plot using Plotly Express\nfig = px.scatter_3d(embedding_df, x='x0', y='x1', z='x2', hover_name='word')\n# Each point represents a word, positioned based on its 'x0', 'x1', and 'x2' coordinates\n# Hovering over a point will show the corresponding word\n\n# Save the plot as an HTML file\noutput_path = \"scatter_plot.html\"\nfig.write_html(output_path)\n\n# Display the plot\nfig.show()\n\nTop 5 words with highest embeddings:\nWord: video, Embedding: [ 5.4536767  -0.08328094  0.03338596]\nWord: factbox, Embedding: [-5.2003880e+00 -1.1863625e-03 -3.2642100e-02]\nWord: trump’s, Embedding: [ 4.952767    0.07439824 -0.00521921]\nWord: obama’s, Embedding: [ 4.718474    0.04242248 -0.06171212]\nWord: gop, Embedding: [ 4.650868   -0.03748338 -0.00787986]\n\n\n\n\n\nPCA Embedding:\n\n“video”: This word has the highest embedding in the PCA plot, with coordinates [5.666067, 0.06447994, 0.02214868]. Its location indicates that it is relatively distinct from other words in the embedding space.\n“factbox”: The word “factbox” has the second-highest embedding, with coordinates [-5.255509, -0.02237466, 0.04613917]. Its negative x-coordinate suggests that it is located on the opposite side of the embedding space compared to the other top words.\n“trump’s”: The word “trump’s” has the third-highest embedding, with coordinates [5.005154, 0.03613936, 0.02672017]. Its proximity to “video” indicates that these words may share some semantic similarities or appear in similar contexts.\n“obama’s”: The word “obama’s” has the fourth-highest embedding, with coordinates [4.778542, 0.02571037, -0.04872589]. Its location near “trump’s” suggests that it is semantically related to this word.\n“gop”: The word “gop” has the fifth-highest embedding, with coordinates [4.7234287, 0.00562899, -0.02563044]. Its proximity to “trump’s” and “obama’s” indicates that it may share some contextual similarities with these words.\n\n\n\n\nT-SNE is a widely used technique for visualizing high-dimensional data in a lower-dimensional space, such as 2D or 3D. It aims to preserve the local structure of the data while revealing global patterns and clusters. By visualizing the word embeddings using t-SNE, we can gain insights into the semantic structure and relationships between words. Words that are semantically similar or related are expected to be clustered together in the t-SNE plot.\n\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nfrom sklearn.manifold import TSNE\n\n# Assuming you have obtained the embedding weights and vocabulary\n# Replace 'weights' and 'vocab' with actual values\nweights = model1.get_layer('embedding_title').get_weights()[0]  # Example embedding weights\nvocab = title_vectorize_layer.get_vocabulary()  # Example vocabulary\n\n# Perform t-SNE for dimensionality reduction\ntsne = TSNE(n_components=3)\nweights_tsne = tsne.fit_transform(weights)\n\n# Extract embeddings for each word\nembedding_dict = {word: embedding for word, embedding in zip(vocab, weights_tsne)}\n\n# Sort words based on embedding values\nsorted_words = sorted(embedding_dict, key=lambda x: np.linalg.norm(embedding_dict[x]), reverse=True)\n\n# Print the top words with highest embeddings\nnum_top_words = 5\nprint(f\"Top {num_top_words} words with highest embeddings:\")\nfor word in sorted_words[:num_top_words]:\n    print(f\"Word: {word}, Embedding: {embedding_dict[word]}\")\n\n# Create DataFrame for plotting\nembedding_df = pd.DataFrame({\n    'word': vocab,\n    'x0': weights_tsne[:, 0],\n    'x1': weights_tsne[:, 1],\n    'x2': weights_tsne[:, 2]\n})\n\n# Set the renderer to \"iframe\"\nimport plotly.io as pio\npio.renderers.default = \"iframe\"\n\n# Create a scatter plot using Plotly Express\nfig = px.scatter_3d(embedding_df, x='x0', y='x1', z='x2', hover_name='word')\n# Each point represents a word, positioned based on its 'x0', 'x1', and 'x2' coordinates\n# Hovering over a point will show the corresponding word\n\n# Save the plot as an HTML file\noutput_path = \"tsne_scatter_plot.html\"\nfig.write_html(output_path)\n\n# Display the plot\nfig.show()\n\nTop 5 words with highest embeddings:\nWord: factbox, Embedding: [-20.716244    1.6520227  11.792851 ]\nWord: rohingya, Embedding: [-20.60748     1.8122349  11.386174 ]\nWord: trumps, Embedding: [-20.589226   1.833724  11.317928]\nWord: macron, Embedding: [-20.500938    1.8953061  11.031187 ]\nWord: catalan, Embedding: [-20.492176    1.8876332  11.009249 ]\n\n\n\n\n\nt-SNE Embedding:\n\n“factbox”: This word has the highest embedding in the t-SNE plot, with coordinates [-20.716244, 1.6520227, 11.792851]. Its location suggests that it is relatively distinct from other words in the embedding space.\n“rohingya”: The word “rohingya” has the second-highest embedding, with coordinates [-20.60748, 1.8122349, 11.386174]. Its proximity to “factbox” indicates that these words may share some semantic similarities or appear in similar contexts.\n“trumps”: The word “trumps” has the third-highest embedding, with coordinates [-20.589226, 1.833724, 11.317928]. Its location near “rohingya” and “factbox” suggests that it is also semantically related to these words.\n“macron”: The word “macron” has the fourth-highest embedding, with coordinates [-20.500938, 1.8953061, 11.031187]. Its proximity to the previous words indicates that it may share some contextual similarities with them.\n“catalan”: The word “catalan” has the fifth-highest embedding, with coordinates [-20.492176, 1.8876332, 11.009249]. Its location close to the other top words suggests that it is semantically related to them.\n\n\n\n\nNow we finish the presenting of the learned word embedding and we show the written text discusses at least 5 words whose location is interpretable within the embedding. Thank you for your time."
  },
  {
    "objectID": "posts/16BHW1/index.html",
    "href": "posts/16BHW1/index.html",
    "title": "PIC 16B HW1",
    "section": "",
    "text": "import pandas as pd\n\n# advanced plotting tools for data frames\n# basically a bunch of matplotlib shortcuts\nimport seaborn as sns \n\nfrom matplotlib import pyplot as plt\nimport numpy as np"
  },
  {
    "objectID": "posts/16BHW1/index.html#section-1-database-creation",
    "href": "posts/16BHW1/index.html#section-1-database-creation",
    "title": "PIC 16B HW1",
    "section": "Section 1: Database Creation",
    "text": "Section 1: Database Creation\nWe are required to create a database with three tables: temperatures, stations, and countries. We need to know how to access country names and relate them to temperature readings. We also need to keep these as three seperate tables in our database.\nLecture Notes: “Databases provide us with a structured way to move subsets of data from storage into memory. Python has a module called sqlite3 (already installed in PIC16B-24W environment) which we can use to create, manipulate, and query databases. There’s also a very handy pandas interface, enabling us to efficiently create pandas data frames containing exactly the data that we want.”\n\nimport sqlite3\nconn = sqlite3.connect(\"temps.db\")\n\n\ndf = pd.read_csv(\"temps.csv\")\ndf_iter = pd.read_csv(\"temps.csv\", chunksize = 100000)\ndf.head()\n\n\n\n\n\n\n\n\nID\nYear\nVALUE1\nVALUE2\nVALUE3\nVALUE4\nVALUE5\nVALUE6\nVALUE7\nVALUE8\nVALUE9\nVALUE10\nVALUE11\nVALUE12\n\n\n\n\n0\nACW00011604\n1961\n-89.0\n236.0\n472.0\n773.0\n1128.0\n1599.0\n1570.0\n1481.0\n1413.0\n1174.0\n510.0\n-39.0\n\n\n1\nACW00011604\n1962\n113.0\n85.0\n-154.0\n635.0\n908.0\n1381.0\n1510.0\n1393.0\n1163.0\n994.0\n323.0\n-126.0\n\n\n2\nACW00011604\n1963\n-713.0\n-553.0\n-99.0\n541.0\n1224.0\n1627.0\n1620.0\n1596.0\n1332.0\n940.0\n566.0\n-108.0\n\n\n3\nACW00011604\n1964\n62.0\n-85.0\n55.0\n738.0\n1219.0\n1442.0\n1506.0\n1557.0\n1221.0\n788.0\n546.0\n112.0\n\n\n4\nACW00011604\n1965\n44.0\n-105.0\n38.0\n590.0\n987.0\n1500.0\n1487.0\n1477.0\n1377.0\n974.0\n31.0\n-178.0\n\n\n\n\n\n\n\n\ndf.shape\n\n(1359937, 14)\n\n\n\ndef prepare_df(df):\n    df = df.set_index(keys=[\"ID\", \"Year\"])\n    df = df.stack()\n    df = df.reset_index()\n    df = df.rename(columns = {\"level_2\"  : \"Month\" , 0 : \"Temp\"})\n    df[\"Month\"] = df[\"Month\"].str[5:].astype(int)\n    df[\"Temp\"]  = df[\"Temp\"] / 100\n    return(df)\n\n\nData Cleaning\n\ndf = prepare_df(df)\ndf.head()\n\n\n\n\n\n\n\n\nID\nYear\nMonth\nTemp\n\n\n\n\n0\nACW00011604\n1961\n1\n-0.89\n\n\n1\nACW00011604\n1961\n2\n2.36\n\n\n2\nACW00011604\n1961\n3\n4.72\n\n\n3\nACW00011604\n1961\n4\n7.73\n\n\n4\nACW00011604\n1961\n5\n11.28\n\n\n\n\n\n\n\n\n\nAdding Temperatures to Our Database\n\ndf_iter = pd.read_csv(\"temps.csv\", chunksize = 100000)\nfor i, df in enumerate(df_iter):\n    df = prepare_df(df)\n    df.to_sql(\"temperatures\", conn, if_exists = \"replace\" if i == 0 else \"append\", index = False)\n\n\n\nAdding Stations to Our Database\n\nurl = \"station-metadata.csv\"\nstations = pd.read_csv(url)\nstations.to_sql(\"stations\", conn, if_exists = \"replace\", index=False)\n\n27585\n\n\n\n\nAdding Countries to Our Database\n\nurl = \"country.csv\"\nstations = pd.read_csv(url)\nstations.to_sql(\"countries\", conn, if_exists = \"replace\", index=False)\n\n279\n\n\n\ndf = pd.read_csv(\"country.csv\")\ndf = df.rename(columns={'Name': 'Country'})\ndf.to_csv(\"country_modified.csv\", index=False)\n\n\n\nLet’s check our Databse:\n\ncursor = conn.cursor()\ncursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\nprint(cursor.fetchall())\n\n[('temperatures',), ('stations',), ('countries',)]\n\n\n\ncursor.execute(\"SELECT sql FROM sqlite_master WHERE type='table';\")\n\nfor result in cursor.fetchall():\n    print(result[0])\n\nCREATE TABLE \"temperatures\" (\n\"ID\" TEXT,\n  \"Year\" INTEGER,\n  \"Month\" INTEGER,\n  \"Temp\" REAL\n)\nCREATE TABLE \"stations\" (\n\"ID\" TEXT,\n  \"LATITUDE\" REAL,\n  \"LONGITUDE\" REAL,\n  \"STNELEV\" REAL,\n  \"NAME\" TEXT\n)\nCREATE TABLE \"countries\" (\n\"FIPS 10-4\" TEXT,\n  \"ISO 3166\" TEXT,\n  \"Name\" TEXT\n)"
  },
  {
    "objectID": "posts/16BHW1/index.html#section-2-write-a-query-function",
    "href": "posts/16BHW1/index.html#section-2-write-a-query-function",
    "title": "PIC 16B HW1",
    "section": "Section 2: Write a Query Function",
    "text": "Section 2: Write a Query Function\nIn this section, we need to first write a climate_database.py file with the function query_climate_database() which accepts five arguments: \n1) db_file: the file name for the database 2) country: a string giving the name of a country for which data should be returned 3) year_begin: integer giving the earliest years for which should be returne  4) year_end: integer giving the latest years for which should be returned 5) month: an integer giving the month of the year for which should be returned\nThe return value of query_climate_database() is a Pandas dataframe of temeperature readings for the specified country, in the specified date range,in the specified month of year. We are required to have these columns: NAME, LATITUDE, LONGITUDE, Country, Year, Month, Temp\n\nfrom climate_database import query_climate_database\nimport inspect\nprint(inspect.getsource(query_climate_database))\n\ndef query_climate_database(db_file, country, year_begin, year_end, month):\n    conn = sqlite3.connect(db_file)\n    \n    query = '''\n    SELECT S.NAME, S.LATITUDE, S.LONGITUDE, C.Name AS COUNTRY, \n       T.Year AS Year, T.Month AS Month, T.Temp AS Temp\n    FROM temperatures T\n    JOIN stations S ON T.ID=S.ID\n    JOIN countries C ON S.ID=SUBSTR(C.ID, 1, LENGTH(S.ID)) -- Assuming ID matching logic\n    WHERE C.Name = ? AND T.Year BETWEEN ? AND ? AND T.Month = ?\n    ''',\n    (country, year_begin, year_end, month)\n\n    df = pd.read_sql_query(query, conn)\n\n    conn.close()\n    \n\n    return df\n\n\n\n\nquery_climate_database(db_file = \"temps.db\",\n                       country = \"India\", \n                       year_begin = 1980, \n                       year_end = 2020,\n                       month = 1)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nName\nYear\nMonth\nTemp\n\n\n\n\n0\nBALDRICK_AWS\n-82.767\n-13.05\nAntarctica\n2008\n4\n-41.08\n\n\n1\nBALDRICK_AWS\n-82.767\n-13.05\nAntarctica\n2008\n5\n-48.40\n\n\n2\nBALDRICK_AWS\n-82.767\n-13.05\nAntarctica\n2008\n6\n-50.70\n\n\n3\nBALDRICK_AWS\n-82.767\n-13.05\nAntarctica\n2008\n7\n-49.14\n\n\n4\nBALDRICK_AWS\n-82.767\n-13.05\nAntarctica\n2008\n10\n-43.66\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n8468\nVIKTORIYA_ISLAND\n80.167\n36.75\nSvalbard\n1995\n8\n0.00\n\n\n8469\nVIKTORIYA_ISLAND\n80.167\n36.75\nSvalbard\n1995\n9\n-2.09\n\n\n8470\nVIKTORIYA_ISLAND\n80.167\n36.75\nSvalbard\n1995\n10\n-8.70\n\n\n8471\nVIKTORIYA_ISLAND\n80.167\n36.75\nSvalbard\n1995\n11\n-20.90\n\n\n8472\nVIKTORIYA_ISLAND\n80.167\n36.75\nSvalbard\n1995\n12\n-25.40\n\n\n\n\n8473 rows × 7 columns\n\n\n\n\nANOTHER APPROACH\n\nimport sqlite3\nimport pandas as pd\n\ndb_file = 'temps.db'\n\n\nconn = sqlite3.connect(db_file)\ncursor = conn.cursor()\n\ncursor.execute('''\nCREATE TABLE IF NOT EXISTS countries (\n    country_id INTEGER PRIMARY KEY,\n    country_name TEXT NOT NULL\n)\n''')\n\ncursor.execute('''\nCREATE TABLE IF NOT EXISTS stations (\n    station_id INTEGER PRIMARY KEY,\n    station_name TEXT NOT NULL,\n    latitude REAL,\n    longitude REAL,\n    country_id INTEGER,\n    FOREIGN KEY (country_id) REFERENCES countries (country_id)\n)\n''')\n\ncursor.execute('''\nCREATE TABLE IF NOT EXISTS temperatures (\n    temperature_id INTEGER PRIMARY KEY,\n    station_id INTEGER,\n    country_id INTEGER,\n    year INTEGER,\n    month INTEGER,\n    temp REAL,\n    FOREIGN KEY (station_id) REFERENCES stations (station_id),\n    FOREIGN KEY (country_id) REFERENCES countries (country_id)\n)\n''')\n\n\nconn.commit()\n\n\ndef load_csv_to_table(csv_file, table_name, conn):\n    df = pd.read_csv(csv_file)\n    df.to_sql(table_name, conn, if_exists='replace', index=False)\n\n\nload_csv_to_table('country.csv', 'countries', conn)\nload_csv_to_table('station-metadata.csv', 'stations', conn)\nload_csv_to_table('temps.csv', 'temperatures', conn)\n\nconn.close()\n\n\ndef query_climate_database(db_file, country, year_begin, year_end, month):\n    conn = sqlite3.connect(db_file)\n    \n    temp_column = f'VALUE{month}'\n    \n    query = f'''\n    SELECT st.NAME, st.LATITUDE, st.LONGITUDE, co.Name AS Country, \n           te.Year, {month} AS Month, te.{temp_column} AS Temp\n    FROM temperatures te\n    INNER JOIN stations st ON te.ID = st.ID\n    INNER JOIN countries co ON co.`FIPS 10-4` = SUBSTR(te.ID, 1, 2)\n    WHERE co.`FIPS 10-4` = (SELECT `FIPS 10-4` FROM countries WHERE Name = \"{country}\")\n    AND te.Year BETWEEN {year_begin} AND {year_end}\n    AND te.{temp_column} IS NOT NULL\n    '''\n    \n    df = pd.read_sql_query(query, conn)\n\n    conn.close()\n    \n\n    return df\n\n\nquery_climate_database(db_file =\"temps.db\", country =\"India\", year_begin = 1980, year_end = 2020,month = 1)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1980\n1\n2348.0\n\n\n1\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1981\n1\n2457.0\n\n\n2\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1982\n1\n2419.0\n\n\n3\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1983\n1\n2351.0\n\n\n4\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1984\n1\n2481.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3147\nDARJEELING\n27.050\n88.270\nIndia\n1983\n1\n510.0\n\n\n3148\nDARJEELING\n27.050\n88.270\nIndia\n1986\n1\n690.0\n\n\n3149\nDARJEELING\n27.050\n88.270\nIndia\n1994\n1\n810.0\n\n\n3150\nDARJEELING\n27.050\n88.270\nIndia\n1995\n1\n560.0\n\n\n3151\nDARJEELING\n27.050\n88.270\nIndia\n1997\n1\n570.0\n\n\n\n\n3152 rows × 7 columns\n\n\n\nWhich works well."
  },
  {
    "objectID": "posts/16BHW1/index.html#section-3-write-a-geographic-scatter-function-for-yearly-temperature-increases",
    "href": "posts/16BHW1/index.html#section-3-write-a-geographic-scatter-function-for-yearly-temperature-increases",
    "title": "PIC 16B HW1",
    "section": "Section 3: Write A Geographic Scatter Function for Yearly Temperature Increases",
    "text": "Section 3: Write A Geographic Scatter Function for Yearly Temperature Increases\nConsider this question: How does the average yearly change in temperature vary within a given country?\nIn this section, we are going to write a function called temperature_coefficient_plot() to generate an interactive geographic scatterplot, constructing using Plotly Express with a point for each station.\n\nNote: Apply () method can be used\n\n\ndf_countries = pd.read_csv('country.csv')\nprint(df_countries.columns.tolist())\ndf_countries\n\n['FIPS 10-4', 'ISO 3166', 'Name']\n\n\n\n\n\n\n\n\n\nFIPS 10-4\nISO 3166\nName\n\n\n\n\n0\nAF\nAF\nAfghanistan\n\n\n1\nAX\n-\nAkrotiri\n\n\n2\nAL\nAL\nAlbania\n\n\n3\nAG\nDZ\nAlgeria\n\n\n4\nAQ\nAS\nAmerican Samoa\n\n\n...\n...\n...\n...\n\n\n274\n-\n-\nWorld\n\n\n275\nYM\nYE\nYemen\n\n\n276\n-\n-\nZaire\n\n\n277\nZA\nZM\nZambia\n\n\n278\nZI\nZW\nZimbabwe\n\n\n\n\n279 rows × 3 columns\n\n\n\n\ndf_temps = pd.read_csv('temps.csv')\nprint(df_temps.columns.tolist())\ndf_temps.head()\n\n['ID', 'Year', 'VALUE1', 'VALUE2', 'VALUE3', 'VALUE4', 'VALUE5', 'VALUE6', 'VALUE7', 'VALUE8', 'VALUE9', 'VALUE10', 'VALUE11', 'VALUE12']\n\n\n\n\n\n\n\n\n\nID\nYear\nVALUE1\nVALUE2\nVALUE3\nVALUE4\nVALUE5\nVALUE6\nVALUE7\nVALUE8\nVALUE9\nVALUE10\nVALUE11\nVALUE12\n\n\n\n\n0\nACW00011604\n1961\n-89.0\n236.0\n472.0\n773.0\n1128.0\n1599.0\n1570.0\n1481.0\n1413.0\n1174.0\n510.0\n-39.0\n\n\n1\nACW00011604\n1962\n113.0\n85.0\n-154.0\n635.0\n908.0\n1381.0\n1510.0\n1393.0\n1163.0\n994.0\n323.0\n-126.0\n\n\n2\nACW00011604\n1963\n-713.0\n-553.0\n-99.0\n541.0\n1224.0\n1627.0\n1620.0\n1596.0\n1332.0\n940.0\n566.0\n-108.0\n\n\n3\nACW00011604\n1964\n62.0\n-85.0\n55.0\n738.0\n1219.0\n1442.0\n1506.0\n1557.0\n1221.0\n788.0\n546.0\n112.0\n\n\n4\nACW00011604\n1965\n44.0\n-105.0\n38.0\n590.0\n987.0\n1500.0\n1487.0\n1477.0\n1377.0\n974.0\n31.0\n-178.0\n\n\n\n\n\n\n\n\ndf_stationmetadata = pd.read_csv('station-metadata.csv')\nprint(df_stationmetadata.columns.tolist())\ndf_stationmetadata\n\n['ID', 'LATITUDE', 'LONGITUDE', 'STNELEV', 'NAME']\n\n\n\n\n\n\n\n\n\nID\nLATITUDE\nLONGITUDE\nSTNELEV\nNAME\n\n\n\n\n0\nACW00011604\n57.7667\n11.8667\n18.0\nSAVE\n\n\n1\nAE000041196\n25.3330\n55.5170\n34.0\nSHARJAH_INTER_AIRP\n\n\n2\nAEM00041184\n25.6170\n55.9330\n31.0\nRAS_AL_KHAIMAH_INTE\n\n\n3\nAEM00041194\n25.2550\n55.3640\n10.4\nDUBAI_INTL\n\n\n4\nAEM00041216\n24.4300\n54.4700\n3.0\nABU_DHABI_BATEEN_AIR\n\n\n...\n...\n...\n...\n...\n...\n\n\n27580\nZI000067983\n-20.2000\n32.6160\n1132.0\nCHIPINGE\n\n\n27581\nZI000067991\n-22.2170\n30.0000\n457.0\nBEITBRIDGE\n\n\n27582\nZIXLT371333\n-17.8300\n31.0200\n1471.0\nHARARE_BELVEDERE\n\n\n27583\nZIXLT443557\n-18.9800\n32.4500\n1018.0\nGRAND_REEF\n\n\n27584\nZIXLT622116\n-19.4300\n29.7500\n1411.0\nGWELO\n\n\n\n\n27585 rows × 5 columns\n\n\n\n\nimport pandas as pd\nimport sqlite3\nimport plotly.express as px\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\ndef temperature_coefficient_plot(db_file, country, year_begin, year_end, month, min_obs, **kwargs):\n    conn = sqlite3.connect(db_file)\n    \n    country_code_query = f\"SELECT `FIPS 10-4` FROM countries WHERE Name = '{country}'\"\n    country_code = pd.read_sql_query(country_code_query, conn).iloc[0, 0]\n    \n\n    temp_column = f'VALUE{month}'\n    \n\n    query = f'''\n    SELECT st.NAME, st.LATITUDE, st.LONGITUDE, '{country}' AS Country, \n           te.Year, {month} AS Month, te.{temp_column} AS Temp\n    FROM temperatures te\n    INNER JOIN stations st ON te.ID = st.ID\n    WHERE SUBSTR(te.ID, 1, 2) = '{country_code}'\n    AND te.Year BETWEEN {year_begin} AND {year_end}\n    AND te.{temp_column} IS NOT NULL\n    '''\n\n    df = pd.read_sql_query(query, conn)\n\n    conn.close()\n    \n\n    df = df.groupby('NAME').filter(lambda x: len(x) &gt;= min_obs)\n    \n\n    def calculate_slope(df):\n        if len(df) &lt; min_obs:\n            return np.nan  \n        X = df['Year'].values.reshape(-1, 1)\n        y = df['Temp'].values\n        reg = LinearRegression().fit(X, y)\n        return reg.coef_[0]\n    \n    df['Slope'] = df.groupby('NAME').apply(calculate_slope)\n    \n    df = df.dropna(subset=['Slope'])\n    \n\n    df = df.drop_duplicates(subset=['NAME'])\n    \n\n    fig = px.scatter_mapbox(df, lat=\"LATITUDE\", lon=\"LONGITUDE\", \n                            color=\"Slope\",\n                            size=np.abs(df['Slope']), \n                            **kwargs)\n    \n\n    if 'color_continuous_scale' not in kwargs:\n        fig.update_traces(marker=dict(colorscale='RdYlGn'))\n    \n    fig.update_layout(mapbox_style=\"carto-positron\", mapbox_zoom=2)\n    fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n    \n    return fig\n\ncolor_map = px.colors.diverging.RdGy_r\n\n\nfig = temperature_coefficient_plot(\n    db_file=\"climate_data.db\", \n    country=\"India\", \n    year_begin=1980, \n    year_end=2020, \n    month=1, \n    min_obs=10,\n    zoom=3,\n    mapbox_style=\"carto-positron\",\n    color_continuous_scale=color_map)\n\n\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json"
  },
  {
    "objectID": "posts/16BHW1/index.html#section-4-create-two-more-interesting-figures",
    "href": "posts/16BHW1/index.html#section-4-create-two-more-interesting-figures",
    "title": "PIC 16B HW1",
    "section": "Section 4: Create Two More Interesting Figures",
    "text": "Section 4: Create Two More Interesting Figures\n\nimport plotly\nplotly.__version__\n\n'5.18.0'\n\n\n\nFigure 1\nQuestion Address: How does the elevation above sea level vary with latitude across different geographical locations?\n\nimport pandas as pd\nfilename = \"station-metadata.csv\"\nstations = pd.read_csv(filename)\nstations= stations.dropna(subset = [\"LATITUDE\", \"STNELEV\"])\n\n\nfrom plotly import express as px\n\nfig = px.scatter(data_frame = stations,\n                 x = \"LATITUDE\",\n                 y = \"STNELEV\",\n                 width = 500,\n                 height = 300,\n                )\n\n#reduce whitespace\nfig.update_layout(margin={\"r\":0, \"t\":0, \"l\":0, \"b\":0})\n# show the plot\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\n\npic1.jpg\n\n\nBy plotting latitude against station elevation, researchers can analyze patterns such as:\nAre higher elevations found at specific latitudes? Is there a correlation between latitude and elevation, and is it positive or negative? How does the distribution of elevations vary across different latitudes? Are there any outliers that suggest unique geographical features?\n\n\nFigure 2\nQuestion Address: How do temperature values for January 1961 vary across different IDs (which could represent different locations or stations) in the dataset provided?\n\nimport pandas as pd\nimport plotly.express as px\n\ndef plot_temperature_boxplot_january_1961(csv_filename):\n    df = pd.read_csv(csv_filename)\n    \n    df_january_1961 = df[df['Year'] == 1961][['ID', 'VALUE1']]\n    \n    # Rename 'VALUE1' to 'Temperature' for clarity\n    df_january_1961.rename(columns={'VALUE1': 'Temperature'}, inplace=True)\n    \n    # Create the boxplot\n    fig = px.box(data_frame=df_january_1961,\n                 x=\"ID\",\n                 y=\"Temperature\",\n                 width=500,\n                 height=300)\n\n    # Reduce whitespace\n    fig.update_layout(margin={\"r\":0, \"t\":50, \"l\":0, \"b\":0})\n    \n    # Show the plot\n    fig.show()\nplot_temperature_boxplot_january_1961('temps.csv')\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\n\npic2.jpg\n\n\nBy constructing a boxplot of January temperatures across different locations, researchers can investigate patterns such as:\nWhat is the typical range of temperatures experienced in various locations during January? Are there any locations with particularly high or low median temperatures compared to others? How does the interquartile range (IQR) of temperatures compare among different locations? Are there any anomalies indicating extreme weather events or microclimates in specific locations?"
  },
  {
    "objectID": "posts/16BHW2/index.html",
    "href": "posts/16BHW2/index.html",
    "title": "PIC 16B HW2",
    "section": "",
    "text": "In today’s tutorial, we are going to learn how to use webscraping to build a “recommender system”, answering the following questions:  What movie or TV shows share a actor with your favorite movie or show? We assume that if Movie Y has many of the same actors as your favorite Movie X, you might also enjoy Y. To see how we develop a webscraper, please scroll down to section 2.\n\n\n\n\n\nWe need to first choose one favorite movie on TMDB page, here I choose “Harry Potter and the Philosopher’s Stone”, with url: https://www.themoviedb.org/movie/671-harry-potter-and-the-philosopher-s-stone/\n\n\n\nLet’s “reherse” what our scraper will do; follong these steps: First, we click on the Full Cast & Crew link, which leads us to the page origianl-url/cast (https://www.themoviedb.org/movie/671-harry-potter-and-the-philosopher-s-stone/cast) in this case. Second, we click on the portrait of one of the actors, which leads us to a different url, introducing the actor’s acting credit, like this: https://www.themoviedb.org/person/10980-daniel-radcliffe  Finally, we stay in this page, and scroll down to examine the actor’s Acting section, seeing the titles of a few movies and TV shows in this section. Remember our scraper is going to replicate the exact same process: “Staring with your favorite movie, it’s going to look at all the actors in that movie, and then log all the other movies of TV shows they worked on” **Note that it would be agood idea to use the Developer Tools to inspect individual HTML elements and look for patterns among the names you are looking for.\n\n\n\nconda activate PIC16B-24W\nscrapy startproject TMDB_scraper\ncd TMDB_scraper\nUse this code to initialize the project\n\n\n\nAdd the folloing line to the file settings.py to prevent scraper from downloading too much data while you’re still testing.\nCLOSESPIDER_PAGECOUNT = 20\nPS: you will remove this line later! PPS: If you run into 403 Forbidden errors latter, you need to change user agent line in setting.py, one way to change user agent on scrapy shell is\nscrapy shell -s USER_AGENT='Scrapy/2.8.0 (+https://scrapy.org)' https://www.themoviedb.org/...\n\n\n\n\nIn this section, we create a file tmdb_spider.py inside the spiders directory called tmdb_spider.py. Note we will write scraper codes in this file.\n\n# to run \n# scrapy crawl tmdb_spider -o movies.csv -a subdir=671-harry-potter-and-the-philosopher-s-stone\n\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    # Give a unique identifier for spider name, used to trigger this certain spider from the command line\n    name = 'tmdb_spider'\n    \n    def __init__(self, subdir=None, *args, **kwargs):\n        \"\"\"\n        Initialize the spider instance with a specific movie's subdirectory.\n        This subdirectory is essential for crafting the starting URL from which the spider begins scraping.\n        \n        :param subdir: Subdirectory for the movie on TMDB site, used to build the start URL\n        :param args: Positional arguments\n        :param kwargs: Keyword arguments\n        \"\"\"\n        super(TmdbSpider, self).__init__(*args, **kwargs)\n        # Set the starting URL to scrape based on the provided movie subdirectory.\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n\n    def parse(self, response):\n        \"\"\"\n        Parses the main movie page, extracts the from the full cast, and makes a request to that link\n        :param response: The response object containing the content of the movies' main page\n        \"\"\"\n        # Extracts the link to the \"full cast and crew\" page from the main movie page\n        full_cast_link = response.css('p.new_button a::attr(href)').get()\n        if full_cast_link:\n            # If the link is found, make a request to the cast list page\n            yield response.follow(full_cast_link, self.parse_full_credits)\n    \n    def parse_full_credits(self, response):\n        \"\"\"\n        Parses the \"Full Cast & Crew\" page, extracts each individual actor's personal page links, and makes requests to those pages.\n        :param response: contains the content of the \"Full Cast & Crew\" page.\n        \"\"\"\n        # Selects only the first \"panel pad\" section of the page using CSS selectors\n        first_panel_pad = response.xpath('(//section[contains(@class, \"panel\") and contains(@class, \"pad\")])[1]')\n        for actor in first_panel_pad.css('ol.people.credits &gt; li'):\n            # Extracts the individual actor page link，not including crew members\n            actor_page = actor.css('a::attr(href)').get()\n            if actor_page:\n                # Makes a request to the actor's personal page, directing go to its acting role section\n                yield response.follow(actor_page + '?credit_department=Acting', self.parse_actor_page)\n                \n    def parse_actor_page(self, response):\n        \"\"\"\n        Parses the actor's personal page, extracting information about the movies or TV shows the actor has participated in.\n        :param response: The response object containing the content of the actor's personal page. \n        We want only the works listed in \"Acting\" section for the actor page.\n        We need to determine both the name of actor and the name of each movie/show.\n        \"\"\"\n        # Extracts the actor's name\n        actor_name = response.css('h2.title a::text').get()\n        # Directly gets all texts of this structure by using xpath a[@class='tooltip']/bdi\n        movies = response.xpath('//a[@class=\"tooltip\"]/bdi/text()').getall()\n        # Extracts the title of the work\n        for title in movies:\n            # Yields a dictionary (two key-value pairs) containing the actor's name and the title of the work\n            yield {\n                'actor': actor_name,\n                'movie_or_TV_name': title\n            }\n\nAfter successfully build the spider, we can run this command in terminal: \nscrapy crawl tmdb_spider -o results.csv -a subdir=671-harry-potter-and-the-philosopher-s-stone \nWhich run the spider and save a results.csv file with columns for actor names and the movies and TV shows on which they featured in.\nOnce the spider is fully written, we can comment out the line  CLOSESPIDER_PAGECOUNT = 20  in the settings.py file, then run this command in the terminal to generate a results.csv  scrapy crawl tmdb_spider -o results.csv -a subdir=671-harry-potter-and-the-philosopher-s-stone\nThis results.csv will contain columns for actor names and the movies and TV shows on which they featured in.\n\n\n\nIn this section, I will introduce visualization of numbers of shared actors.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\n\ndata=pd.read_csv('results.csv')\nprint(data)\n\n                 actor                    movie_or_TV_name\n0     Daniel Radcliffe  Have I Got a Bit More News for You\n1     Daniel Radcliffe     David Holmes: The Boy Who Lived\n2     Daniel Radcliffe           100 Years of Warner Bros.\n3     Daniel Radcliffe                            Mulligan\n4     Daniel Radcliffe                             Digman!\n...                ...                                 ...\n2956      Rupert Grint                            The View\n2957      Rupert Grint                                GMTV\n2958      Rupert Grint      The Tonight Show with Jay Leno\n2959      Rupert Grint                 An Audience with...\n2960      Rupert Grint                               Today\n\n[2961 rows x 2 columns]\n\n\n\nprint(data.head(35))\n\n               actor                                   movie_or_TV_name\n0   Daniel Radcliffe                 Have I Got a Bit More News for You\n1   Daniel Radcliffe                    David Holmes: The Boy Who Lived\n2   Daniel Radcliffe                          100 Years of Warner Bros.\n3   Daniel Radcliffe                                           Mulligan\n4   Daniel Radcliffe                                            Digman!\n5   Daniel Radcliffe                                     Extrapolations\n6   Daniel Radcliffe                       Weird: The Al Yankovic Story\n7   Daniel Radcliffe                                      The Lost City\n8   Daniel Radcliffe  Harry Potter 20th Anniversary: Return to Hogwarts\n9   Daniel Radcliffe                         (K)nox: The Rob Knox Story\n10  Daniel Radcliffe  Unbreakable Kimmy Schmidt: Kimmy vs. the Music...\n11  Daniel Radcliffe  Unbreakable Kimmy Schmidt: Kimmy vs. the Reverend\n12  Daniel Radcliffe                     Endgame & Rough for Theatre II\n13  Daniel Radcliffe                               Escape from Pretoria\n14  Daniel Radcliffe                                        Guns Akimbo\n15  Daniel Radcliffe                            The Kelly Clarkson Show\n16  Daniel Radcliffe                               Playmobil: The Movie\n17  Daniel Radcliffe                                    Miracle Workers\n18  Daniel Radcliffe                                    Beast of Burden\n19  Daniel Radcliffe                                      2 Dope Queens\n20  Daniel Radcliffe  The Robot Chicken Walking Dead Special: Die wa...\n21  Daniel Radcliffe                                             Jungle\n22  Daniel Radcliffe  National Theatre Live: Rosencrantz & Guildenst...\n23  Daniel Radcliffe                                     Lost in London\n24  Daniel Radcliffe                                           Imperium\n25  Daniel Radcliffe                                     Swiss Army Man\n26  Daniel Radcliffe                                   Now You See Me 2\n27  Daniel Radcliffe                                Victor Frankenstein\n28  Daniel Radcliffe                                   The Gamechangers\n29  Daniel Radcliffe                 The Late Show with Stephen Colbert\n30  Daniel Radcliffe                                         Trainwreck\n31  Daniel Radcliffe                     Tom Felton Meets the Superfans\n32  Daniel Radcliffe                                           Hot Ones\n33  Daniel Radcliffe                                    BoJack Horseman\n34  Daniel Radcliffe                                 Trailblazer Honors\n\n\n\n\"\"\"\nCounts the occurrences of each unique value in the 'actor' column of the dataframe 'data' and returns a Series.\nThe index of the Series will be the actor names, and the values will be the count of movies each actor has appeared in.\n\"\"\"\nactor_counts = data['actor'].value_counts()\n\n\n\n\n\nactor_counts\n\nactor\nJohn Cleese          241\nJohn Hurt            233\nJulie Walters        152\nRobbie Coltrane      150\nLeslie Phillips      134\n                    ... \nBen Borowiecki         2\nEmily Dale             2\nWill Theakston         2\nLeilah Sutherland      1\nSaunders Triplets      1\nName: count, Length: 63, dtype: int64\n\n\n\n\"\"\"\nThis script visualizes the number of movies each actor has appeared in using a bar plot.\n\"\"\"\n\n# Creates a new figure with a specified size.\nplt.figure(figsize=(12,6))\n\n# Creates a bar plot using seaborn. The x-axis represents actors, and the y-axis represents the count of movies.\n# `actor_counts` is assumed to be a pandas Series where the index contains actor names and values represent movie counts.\nsns.barplot(x=actor_counts.index, y=actor_counts.values)\n\n# Rotates the x-axis labels (actor names) by 90 degrees to prevent overlap and improve readability.\nplt.xticks(rotation=90)\n\n# Sets the label for the x-axis as 'Actor'.\nplt.xlabel('Actor')\n\n# Sets the label for the y-axis as 'Movie Count'.\nplt.ylabel('Movie Count')\n\n# Sets the title of the plot as 'Number of Movies for Each Actor'.\nplt.title('Number of Movies for Each Actor')\n\n# Adjusts the layout to make sure everything fits within the figure area without any clipping.\nplt.tight_layout()\n\n# Displays the plot.\nplt.show()"
  },
  {
    "objectID": "posts/16BHW2/index.html#introduction",
    "href": "posts/16BHW2/index.html#introduction",
    "title": "PIC 16B HW2",
    "section": "",
    "text": "In today’s tutorial, we are going to learn how to use webscraping to build a “recommender system”, answering the following questions:  What movie or TV shows share a actor with your favorite movie or show? We assume that if Movie Y has many of the same actors as your favorite Movie X, you might also enjoy Y. To see how we develop a webscraper, please scroll down to section 2."
  },
  {
    "objectID": "posts/16BHW2/index.html#setup",
    "href": "posts/16BHW2/index.html#setup",
    "title": "PIC 16B HW2",
    "section": "",
    "text": "We need to first choose one favorite movie on TMDB page, here I choose “Harry Potter and the Philosopher’s Stone”, with url: https://www.themoviedb.org/movie/671-harry-potter-and-the-philosopher-s-stone/\n\n\n\nLet’s “reherse” what our scraper will do; follong these steps: First, we click on the Full Cast & Crew link, which leads us to the page origianl-url/cast (https://www.themoviedb.org/movie/671-harry-potter-and-the-philosopher-s-stone/cast) in this case. Second, we click on the portrait of one of the actors, which leads us to a different url, introducing the actor’s acting credit, like this: https://www.themoviedb.org/person/10980-daniel-radcliffe  Finally, we stay in this page, and scroll down to examine the actor’s Acting section, seeing the titles of a few movies and TV shows in this section. Remember our scraper is going to replicate the exact same process: “Staring with your favorite movie, it’s going to look at all the actors in that movie, and then log all the other movies of TV shows they worked on” **Note that it would be agood idea to use the Developer Tools to inspect individual HTML elements and look for patterns among the names you are looking for.\n\n\n\nconda activate PIC16B-24W\nscrapy startproject TMDB_scraper\ncd TMDB_scraper\nUse this code to initialize the project\n\n\n\nAdd the folloing line to the file settings.py to prevent scraper from downloading too much data while you’re still testing.\nCLOSESPIDER_PAGECOUNT = 20\nPS: you will remove this line later! PPS: If you run into 403 Forbidden errors latter, you need to change user agent line in setting.py, one way to change user agent on scrapy shell is\nscrapy shell -s USER_AGENT='Scrapy/2.8.0 (+https://scrapy.org)' https://www.themoviedb.org/..."
  },
  {
    "objectID": "posts/16BHW2/index.html#write-scraper",
    "href": "posts/16BHW2/index.html#write-scraper",
    "title": "PIC 16B HW2",
    "section": "",
    "text": "In this section, we create a file tmdb_spider.py inside the spiders directory called tmdb_spider.py. Note we will write scraper codes in this file.\n\n# to run \n# scrapy crawl tmdb_spider -o movies.csv -a subdir=671-harry-potter-and-the-philosopher-s-stone\n\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    # Give a unique identifier for spider name, used to trigger this certain spider from the command line\n    name = 'tmdb_spider'\n    \n    def __init__(self, subdir=None, *args, **kwargs):\n        \"\"\"\n        Initialize the spider instance with a specific movie's subdirectory.\n        This subdirectory is essential for crafting the starting URL from which the spider begins scraping.\n        \n        :param subdir: Subdirectory for the movie on TMDB site, used to build the start URL\n        :param args: Positional arguments\n        :param kwargs: Keyword arguments\n        \"\"\"\n        super(TmdbSpider, self).__init__(*args, **kwargs)\n        # Set the starting URL to scrape based on the provided movie subdirectory.\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n\n    def parse(self, response):\n        \"\"\"\n        Parses the main movie page, extracts the from the full cast, and makes a request to that link\n        :param response: The response object containing the content of the movies' main page\n        \"\"\"\n        # Extracts the link to the \"full cast and crew\" page from the main movie page\n        full_cast_link = response.css('p.new_button a::attr(href)').get()\n        if full_cast_link:\n            # If the link is found, make a request to the cast list page\n            yield response.follow(full_cast_link, self.parse_full_credits)\n    \n    def parse_full_credits(self, response):\n        \"\"\"\n        Parses the \"Full Cast & Crew\" page, extracts each individual actor's personal page links, and makes requests to those pages.\n        :param response: contains the content of the \"Full Cast & Crew\" page.\n        \"\"\"\n        # Selects only the first \"panel pad\" section of the page using CSS selectors\n        first_panel_pad = response.xpath('(//section[contains(@class, \"panel\") and contains(@class, \"pad\")])[1]')\n        for actor in first_panel_pad.css('ol.people.credits &gt; li'):\n            # Extracts the individual actor page link，not including crew members\n            actor_page = actor.css('a::attr(href)').get()\n            if actor_page:\n                # Makes a request to the actor's personal page, directing go to its acting role section\n                yield response.follow(actor_page + '?credit_department=Acting', self.parse_actor_page)\n                \n    def parse_actor_page(self, response):\n        \"\"\"\n        Parses the actor's personal page, extracting information about the movies or TV shows the actor has participated in.\n        :param response: The response object containing the content of the actor's personal page. \n        We want only the works listed in \"Acting\" section for the actor page.\n        We need to determine both the name of actor and the name of each movie/show.\n        \"\"\"\n        # Extracts the actor's name\n        actor_name = response.css('h2.title a::text').get()\n        # Directly gets all texts of this structure by using xpath a[@class='tooltip']/bdi\n        movies = response.xpath('//a[@class=\"tooltip\"]/bdi/text()').getall()\n        # Extracts the title of the work\n        for title in movies:\n            # Yields a dictionary (two key-value pairs) containing the actor's name and the title of the work\n            yield {\n                'actor': actor_name,\n                'movie_or_TV_name': title\n            }\n\nAfter successfully build the spider, we can run this command in terminal: \nscrapy crawl tmdb_spider -o results.csv -a subdir=671-harry-potter-and-the-philosopher-s-stone \nWhich run the spider and save a results.csv file with columns for actor names and the movies and TV shows on which they featured in.\nOnce the spider is fully written, we can comment out the line  CLOSESPIDER_PAGECOUNT = 20  in the settings.py file, then run this command in the terminal to generate a results.csv  scrapy crawl tmdb_spider -o results.csv -a subdir=671-harry-potter-and-the-philosopher-s-stone\nThis results.csv will contain columns for actor names and the movies and TV shows on which they featured in."
  },
  {
    "objectID": "posts/16BHW2/index.html#make-your-recommendations-visualization",
    "href": "posts/16BHW2/index.html#make-your-recommendations-visualization",
    "title": "PIC 16B HW2",
    "section": "",
    "text": "In this section, I will introduce visualization of numbers of shared actors.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\n\ndata=pd.read_csv('results.csv')\nprint(data)\n\n                 actor                    movie_or_TV_name\n0     Daniel Radcliffe  Have I Got a Bit More News for You\n1     Daniel Radcliffe     David Holmes: The Boy Who Lived\n2     Daniel Radcliffe           100 Years of Warner Bros.\n3     Daniel Radcliffe                            Mulligan\n4     Daniel Radcliffe                             Digman!\n...                ...                                 ...\n2956      Rupert Grint                            The View\n2957      Rupert Grint                                GMTV\n2958      Rupert Grint      The Tonight Show with Jay Leno\n2959      Rupert Grint                 An Audience with...\n2960      Rupert Grint                               Today\n\n[2961 rows x 2 columns]\n\n\n\nprint(data.head(35))\n\n               actor                                   movie_or_TV_name\n0   Daniel Radcliffe                 Have I Got a Bit More News for You\n1   Daniel Radcliffe                    David Holmes: The Boy Who Lived\n2   Daniel Radcliffe                          100 Years of Warner Bros.\n3   Daniel Radcliffe                                           Mulligan\n4   Daniel Radcliffe                                            Digman!\n5   Daniel Radcliffe                                     Extrapolations\n6   Daniel Radcliffe                       Weird: The Al Yankovic Story\n7   Daniel Radcliffe                                      The Lost City\n8   Daniel Radcliffe  Harry Potter 20th Anniversary: Return to Hogwarts\n9   Daniel Radcliffe                         (K)nox: The Rob Knox Story\n10  Daniel Radcliffe  Unbreakable Kimmy Schmidt: Kimmy vs. the Music...\n11  Daniel Radcliffe  Unbreakable Kimmy Schmidt: Kimmy vs. the Reverend\n12  Daniel Radcliffe                     Endgame & Rough for Theatre II\n13  Daniel Radcliffe                               Escape from Pretoria\n14  Daniel Radcliffe                                        Guns Akimbo\n15  Daniel Radcliffe                            The Kelly Clarkson Show\n16  Daniel Radcliffe                               Playmobil: The Movie\n17  Daniel Radcliffe                                    Miracle Workers\n18  Daniel Radcliffe                                    Beast of Burden\n19  Daniel Radcliffe                                      2 Dope Queens\n20  Daniel Radcliffe  The Robot Chicken Walking Dead Special: Die wa...\n21  Daniel Radcliffe                                             Jungle\n22  Daniel Radcliffe  National Theatre Live: Rosencrantz & Guildenst...\n23  Daniel Radcliffe                                     Lost in London\n24  Daniel Radcliffe                                           Imperium\n25  Daniel Radcliffe                                     Swiss Army Man\n26  Daniel Radcliffe                                   Now You See Me 2\n27  Daniel Radcliffe                                Victor Frankenstein\n28  Daniel Radcliffe                                   The Gamechangers\n29  Daniel Radcliffe                 The Late Show with Stephen Colbert\n30  Daniel Radcliffe                                         Trainwreck\n31  Daniel Radcliffe                     Tom Felton Meets the Superfans\n32  Daniel Radcliffe                                           Hot Ones\n33  Daniel Radcliffe                                    BoJack Horseman\n34  Daniel Radcliffe                                 Trailblazer Honors\n\n\n\n\"\"\"\nCounts the occurrences of each unique value in the 'actor' column of the dataframe 'data' and returns a Series.\nThe index of the Series will be the actor names, and the values will be the count of movies each actor has appeared in.\n\"\"\"\nactor_counts = data['actor'].value_counts()\n\n\n\n\n\nactor_counts\n\nactor\nJohn Cleese          241\nJohn Hurt            233\nJulie Walters        152\nRobbie Coltrane      150\nLeslie Phillips      134\n                    ... \nBen Borowiecki         2\nEmily Dale             2\nWill Theakston         2\nLeilah Sutherland      1\nSaunders Triplets      1\nName: count, Length: 63, dtype: int64\n\n\n\n\"\"\"\nThis script visualizes the number of movies each actor has appeared in using a bar plot.\n\"\"\"\n\n# Creates a new figure with a specified size.\nplt.figure(figsize=(12,6))\n\n# Creates a bar plot using seaborn. The x-axis represents actors, and the y-axis represents the count of movies.\n# `actor_counts` is assumed to be a pandas Series where the index contains actor names and values represent movie counts.\nsns.barplot(x=actor_counts.index, y=actor_counts.values)\n\n# Rotates the x-axis labels (actor names) by 90 degrees to prevent overlap and improve readability.\nplt.xticks(rotation=90)\n\n# Sets the label for the x-axis as 'Actor'.\nplt.xlabel('Actor')\n\n# Sets the label for the y-axis as 'Movie Count'.\nplt.ylabel('Movie Count')\n\n# Sets the title of the plot as 'Number of Movies for Each Actor'.\nplt.title('Number of Movies for Each Actor')\n\n# Adjusts the layout to make sure everything fits within the figure area without any clipping.\nplt.tight_layout()\n\n# Displays the plot.\nplt.show()"
  },
  {
    "objectID": "posts/16BHW4/index.html",
    "href": "posts/16BHW4/index.html",
    "title": "PIC16B HW4",
    "section": "",
    "text": "In this tutorial, four approaches of simulating a two-dimensional heat diffusion will be introduced: matrix multiplication, sparse matrix in JAX, direction operation with numpy, and with jax.\n\n\n\n\n\nN = 101\nepsilon = 0.2\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport time\nimport jax\nfrom jax.experimental import sparse\nimport jax.numpy as jnp\nfrom jax import jit\nimport inspect\n\n\n# This is the initial condition\n# Put 1 unit of heat at midpoint\nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\nplt.imshow(u0)\n\n\n\n\n\n\n\n\n\n\n\n\nWe will first uses matrix-vector multiplication to simulate the heat diffusion in the 2D space. The vector here is created by flattening the current solution: \\(u_k^{i,j}\\)\nEach iteration of the update is given by:\n\ndef advance_time_matvecmul(A, u, epsilon):\n    \"\"\"Advances the simulation by one timestep, via matrix-vector multiplication\n    Args:\n        A: The 2d finite difference matrix, N^2 x N^2.\n        u: N x N grid state at timestep k.\n        epsilon: stability constant.\n\n    Returns:\n        N x N Grid state at timestep k+1.\n    \"\"\"\n    N = u.shape[0]\n    u = u + epsilon * (A @ u.flatten()).reshape((N, N))\n    return u\n\nWe actually view \\(u_k^{i,j}\\) as the element with index N x i + j. The matrix A has the size of N2xN2, without all zeros or all zero columns\n\n#The corresponding matrix A:\nn = N * N\ndiagonals = [-4 * np.ones(n), np.ones(n-1), np.ones(n-1), np.ones(n-N), np.ones(n-N)]\ndiagonals[1][(N-1)::N] = 0\ndiagonals[2][(N-1)::N] = 0\nA = np.diag(diagonals[0]) + np.diag(diagonals[1], 1) + np.diag(diagonals[2], -1) + np.diag(diagonals[3], N) + np.diag(diagonals[4], -N)\n\nWe will define a function get_A(N), which takes the value “N” as the argument and returns the corresponding matrix A in heat_equation.py (put the function you defined in a py. file).\n\ndef get_A(N):\n    \"\"\"The function get_A(N) takes the value N as the argument and returns the output matrix A with matrix multiplication\n    \"\"\"\n    n = N * N\n    diagonals = [-4 * np.ones(n), np.ones(n-1), np.ones(n-1), np.ones(n-N), np.ones(n-N)]\n    diagonals[1][(N-1)::N] = 0\n    diagonals[2][(N-1)::N] = 0\n    A = np.diag(diagonals[0]) + np.diag(diagonals[1], 1) + np.diag(diagonals[2], -1) + np.diag(diagonals[3], N) + np.diag(diagonals[4], -N)\n    return A\n\nThen with the get_A() and advance_time_matvecmul(), we run the code for 2700 iterations. We introduce time module previously in import libraries to help us obvserve how long does it takes.\n\nu = [u0] # Initialize a list 'u' with the initial grid state 'u0'\n\nA = get_A(N) # # Generate the Matrix A using the function 'get_A' with the grid size 'N'\nstart_time = time.time() # Record the starting time of the simulation\nfor i in range (1,2701): # Loop for 2700 iterations\n    # 'i' takes values from 1 to 2700 (inclusive)\n    u.append(advance_time_matvecmul(A, u[-1], epsilon)) #The updated grid state is appended to the list 'u' using the 'append' method\n\nprint(time.time() - start_time) # Calculate and print the elapsed time\n\n73.26655387878418\n\n\nWe observe this method 1 runs for 73 seconds. One thing for sure: this method runs excruciatingly slow.\nWe still need to visualize the diffusion of heat every 300 iterations. We present them in 3 by 3 grid of 2D heat maps\n\nfig, axs = plt.subplots(nrows=3, ncols=3, figsize=(12, 8))\n\"\"\"Create a figure and a 3x3 grid of subplots\n'figsize' specifies the width and height of the figure in inches\n\"\"\"\n\naxs = axs.flatten() # Flatten the 2D array of Axes objects into a 1D array\n# This makes it easier to access individual subplots using a single index\nsubplot_index = 0 # Initialize a variable to keep track of the current subplot index\n\nfor i in range(1, 2701):\n    if i % 300 == 0: # Check if the current iteration is divisible by 300\n        axs[subplot_index].imshow(u[i-1]) # Display the grid state at the current iteration minus 1 (u[i-1])\n\n        num_xticks = 6\n        xtick_locs = np.linspace(0, u[i-1].shape[1]-1, num_xticks)\n        xtick_labels = np.linspace(0, 100, num_xticks).astype(int)\n        axs[subplot_index].set_xticks(xtick_locs) # Set the locations of the x-axis ticks for the current subplot\n        axs[subplot_index].set_xticklabels(xtick_labels) # Set the labels for the x-axis ticks of the current subplot\n        axs[subplot_index].set_title(f\"Iteration={i}\")\n        \"\"\"Set the title of the current subplot\n        The title indicates the current iteration number\"\"\"\n        subplot_index += 1 # Increment the subplot index to move to the next subplot\n        if subplot_index &gt;= 9:\n            \"\"\"Check if all 9 subplots have been filled\n            If true, exit the loop using 'break'\"\"\"\n            break\n\nplt.subplots_adjust(wspace=0.3, hspace=0.3) # Adjust the spacing between subplots\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nWith a sparsed matrix, matrix multiplication will be much faster since the use of batched coordinate (BCOO) only take O(N^2) time for each update.\nWe will define a function get_sparse_A(N), which returns A_sp_matrix, a matrix A in a sparse format, given N in heat_equation.py. At the same time, we repeate Part 1 using get_A_sparse() and the jit-ed version of advance_time_matvecmul.\n\n# Converting initial condition to JAX array\nu0 = jnp.zeros((N, N)) # Initialize the grid to zero\nu0 = u0.at[int(N/2), int(N/2)].set(1.0) # Set the middle center element to 1.0\n\ndef get_sparse_A(N):\n    \"\"\"Construct the sparse Laplacian matrix for the 2D finite difference grid.\n        N: The size of the grid (N x N).\n        A_sparse: The sparse Laplacian matrix in BCOO format.\n    \"\"\"\n    n = N * N # Total number of grid points\n    \n    # Define the diagonals for the Laplacian matrix\n    diagonals = [-4 * jnp.ones(n), jnp.ones(n-1), jnp.ones(n-1), jnp.ones(n-N), jnp.ones(n-N)]\n    diagonals[1] = diagonals[1].at[(N-1)::N].set(0)\n    diagonals[2] = diagonals[2].at[(N-1)::N].set(0) # Adjust for the grid boundary\n\n    # Use JAX to construct the dense Laplacian matrix and convert to sparse format\n    A_dense = jnp.diag(diagonals[0]) + jnp.diag(diagonals[1], 1) + jnp.diag(diagonals[2], -1) + jnp.diag(diagonals[3], N) + jnp.diag(diagonals[4], -N)\n    A_sparse = sparse.BCOO.fromdense(A_dense) # Convert to sparse BCOO format\n    return A_sparse\n\n@jit\ndef advance_time_matvecmul(A, u, epsilon):\n    \"\"\"Advances the simulation by one timestep, via matrix-vector multiplication\n        A: The 2d finite difference matrix, N^2 x N^2.\n        u: N x N grid state at timestep k.\n        epsilon: stability constant.\n        N x N Grid state at timestep k+1.\n    \"\"\"\n    N = u.shape[0] # Extract the grid dimension from the current state\n    u = u + epsilon * (A @ u.flatten()).reshape((N, N)) # Scaled change is added to the current state u to produce the updated state\n    \"\"\"\n    Compute the update for the grid state using matrix-vector multiplication\n    u.flatten(): this converts the 2D array into a 1D array\n    A@u.flatten: @ is the matrix multiplication operator\n    epislon: scales the result of matrix-vector multiplication and subsequent reshaping\n    The result is reshaped back to an N x N grid\n    The scaled change is added to the current state u to produce the updated stat\n    \"\"\"\n    return u\n\nThen with the get_sparse_A() and jit-ed version of advance_time_matvecmul(), we run the code for 2700 iterations. Let’s observe how long does it takes.\n\n# We still introduce time module to help to calculate the elapsed time\nu = [u0]\nA = get_sparse_A(N)\nstart_time = time.time()\nfor i in range (1,2701):\n    u.append(advance_time_matvecmul(A, u[-1], epsilon))\nprint(time.time() - start_time) \n\n0.8678162097930908\n\n\nWe observe this method 2 runs for 0.867 seconds! Since method 1 runs for 73 seconds, method 2 is 84 times faster than method 1.\nNow let’s visualize the diffusion of heat every 300 iterations. We present them in 3 by 3 grid of 2D heat maps\n\n# We still use this way to plot graphs\nfig, axs = plt.subplots(nrows=3, ncols=3, figsize=(12, 8))\naxs = axs.flatten()\nsubplot_index = 0\n\nfor i in range(1, 2701):\n    if i % 300 == 0:\n        axs[subplot_index].imshow(u[i-1])\n\n        num_xticks = 6\n        xtick_locs = np.linspace(0, u[i-1].shape[1]-1, num_xticks)\n        xtick_labels = np.linspace(0, 100, num_xticks).astype(int)\n        axs[subplot_index].set_xticks(xtick_locs)\n        axs[subplot_index].set_xticklabels(xtick_labels)\n        axs[subplot_index].set_title(f\"Iteration={i}\")\n        subplot_index += 1\n        if subplot_index &gt;= 9:\n            break\n\nplt.subplots_adjust(wspace=0.3, hspace=0.3)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nMatrix-vector multiplication approach is not absolutely necessary in terms of computation. With vectorized array operations like np. roll(), the operations could be simpler.\nWe can then write a function advance_time_numpy(u, epsilon) that advances the solution by one timestep. We could pay zeroes to the input array to form an (N+2)x(N=2) array internally, but the argument and the returned solution should still be N x N.\n\ndef advance_time_numpy(u, epsilon):\n    \"\"\"\n    The function computes the Laplacian of the input grid 'u' via vectorized opertaions,\n    simulating a diffusion process. We use Euler method as the applied update rule.\n    \n    u: The input grid representing the current state of the system.\n    epsilon: The diffusion coefficient controlling the rate of diffusion.\n    u_next: The updated grid after one time step of the diffusion process.\n    \"\"\"\n    N = u.shape[0]  \n    \"\"\"Extract the size of the grid (assuming a square grid)\n    Pad the input array with zeros on all sides for boundary conditions\n    This ensures that the boundary values remain zero during the diffusion process\"\"\"\n    \n    u_padded = np.pad(u, 1, mode='constant', constant_values=0)\n    # Compute the Laplacian of u using vectorized operations\n    laplacian_u = (np.roll(u_padded, 1, axis=0)  # Shift the array one step down\n                   + np.roll(u_padded, -1, axis=0) \n                   + np.roll(u_padded, 1, axis=1) # Shift the array one step to the right\n                   + np.roll(u_padded, -1, axis=1) \n                   -4 * u_padded)[1:-1, 1:-1] # Extract the inner part of the array (excluding the padded boundary)\n    \"\"\" We use np.roll to shift the padded array in four directions (up, down, left, right)\n    and then sum the shifted arrays to compute the Laplacian\"\"\"\n    \n    u_next = u + epsilon * laplacian_u \n    \"\"\" Update the grid \"u\" based on the Laplacian and the diffusion coefficient 'epsilon'\n    The update rule is: u_next = u + epsilon * laplacian_u \"\"\"\n    return u_next\n\nThen with the advance_time_numpy(u,epsilon), we run the code for 2700 iterations. Let’s obvserve how long does it takes.\n\n\nu = u0\nstart_time = time.time()\nfor t in range(1, 2701):\n    u = advance_time_numpy(u, epsilon) # Update the grid state 'u' using the 'advance_time_numpy' function\n    \nprint(time.time() - start_time)\n\n0.21011614799499512\n\n\nIt only takes 0.21 s to run the method 3, method 1 takes 73s, it’s 347x faster than method 1\nNow let’s visualize the diffusion of heat every 300 iterations. We present them in 3 by 3 grid of 2D heat maps\n\nu = u0\nfig, axs = plt.subplots(3, 3, figsize=(9, 9))\naxs = axs.flatten()\nsubplot_index = 0\n\nfor i in range(1, 2701):\n    u = advance_time_numpy(u, epsilon)\n    # Update the grid state 'u' using the 'advance_time_numpy' function\n    if i % 300 == 0:\n        axs[subplot_index].imshow(u)\n        axs[subplot_index].set_title(f\"Iteration {i}\")\n\n        num_xticks = 6\n        xtick_locs = np.linspace(0, u.shape[1]-1, num_xticks)\n        xtick_labels = np.linspace(0, 100, num_xticks).astype(int)\n        axs[subplot_index].set_xticks(xtick_locs)\n        axs[subplot_index].set_xticklabels(xtick_labels)\n\n        subplot_index += 1\n\n        if subplot_index &gt;= 9:\n            break\n\nplt.tight_layout()\nplt.subplots_adjust(wspace=0.3, hspace=0.3)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nNow, we will use jax to do the similar using just-in-time compilation.\nWe will now define a function advance_time_jax(u, epsilon) but without using (sparse) matrix multiplication routines. It will be simple if we use the function advance_time_numpy() as the starting point. Keep in mind that jax does not support index assignment.”\n\nimport jax.numpy as jnp\nfrom jax import jit\n\n@jit\ndef advance_time_jax(u, epsilon):\n    \"\"\"\n    Advances the simulation state by one time step using the Laplacian operator and Euler's method.\n    \n    This function computes the Laplacian of the input array 'u' using direct JAX operations for faster execution.\n    It then updates the simulation state based on the computed Laplacian and a given 'epsilon' value, which\n    represents the time step size or the diffusion coefficient.\n\n    \"\"\"\n    N = u.shape[0]\n    \"\"\"Pad the input array with zeros to handle edge conditions\n    The 'mode' argument specifies the padding mode, which is set to 'constant'\n    The 'constant_values' argument specifies the value to be used for padding, which is set to 0.\"\"\"\n    u_padded = jnp.pad(u, 1, mode='constant', constant_values=0)\n\n    # Compute the Laplacian using vectorized operations\n    laplacian_u = (jnp.roll(u_padded, 1, axis=0) + jnp.roll(u_padded, -1, axis=0) +\n                   jnp.roll(u_padded, 1, axis=1) + jnp.roll(u_padded, -1, axis=1) -\n                   4 * u_padded)[1:-1, 1:-1]\n    \n   # Update the simulation state based on the computed Laplacian and the given 'epsilon' value.\n    u_next = u + epsilon * laplacian_u\n    return u_next\n\nThen with the advance_time_jax(u,epsilon), we run the code for 2700 iterations. Let’s obvserve how long does it takes.\n\nu = u0\nstart_time = time.time()\nfor i in range(1, 2701):\n    u = advance_time_jax(u, epsilon)  # Update the grid state 'u' using the 'advance_time_jax' function\nprint(time.time() - start_time)\n\n0.10694193840026855\n\n\nIt only takes 0.10 s to run the method 4. We know method 3 takes 0.21s, method 4 is about 2x faster than method 3.\nNow let’s visualize the diffusion of heat every 300 iterations. We present them in 3 by 3 grid of 2D heat maps\n\nu = u0\nfig, axs = plt.subplots(3, 3, figsize=(9, 9))\naxs = axs.flatten()\nsubplot_index = 0\n\nfor i in range(1, 2701):\n    u = advance_time_jax(u, epsilon) # # Update the grid state 'u' using the 'advance_time_jax' function\n    if i % 300 == 0:\n        axs[subplot_index].imshow(u)\n        axs[subplot_index].set_title(f\"Iteration {i}\")\n\n        num_xticks = 6\n        xtick_locs = np.linspace(0, u.shape[1]-1, num_xticks)\n        xtick_labels = np.linspace(0, 100, num_xticks).astype(int)\n        axs[subplot_index].set_xticks(xtick_locs)\n        axs[subplot_index].set_xticklabels(xtick_labels)\n\n        num_yticks = 6\n        ytick_locs = np.linspace(0, u.shape[0]-1, num_yticks)\n        ytick_labels = np.linspace(0, 100, num_yticks).astype(int)\n        axs[subplot_index].set_yticks(ytick_locs)\n        axs[subplot_index].set_yticklabels(ytick_labels)\n\n        subplot_index += 1\n\n        if subplot_index &gt;= 9:\n            break\n\nplt.tight_layout()\nplt.subplots_adjust(wspace=0.3, hspace=0.3)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nNow let’s compare the performances of four models. Let’s summarize which one is fastest, and which one is easier for us to write.\n\n\n\nModel\nTime\n\n\n\n\nModel 1\n73s\n\n\nModel 2\n0.86s\n\n\nModel 3\n0.21s\n\n\nModel 4\n0.10s\n\n\n\nPerformance Comparison: The matrix multiplication approach (Approach 1) is the slowest among the given methods. Using a sparse matrix with JAX (Approach 2) provides a significant speedup, approximately 80 times faster than the matrix multiplication approach. Utilizing np.roll() (Approach 3) simplifies the computation for the CPU by eliminating the heat points around and adding them together, resulting in improved performance compared to the matrix multiplication approach. JAX (Approach 4) generally offers better performance compared to NumPy.\nSpeed of Implementation: Method 4 is the fastest and the most performant\nEase of Implementation: Method one appears to be the easiest to implement since the code is already provided. The other approaches may require additional understanding of sparse matrices, JIT compilation, and vectorized operations.\n\n\n\n\n\n\n\n\nApproach\nPros\nCons\n\n\n\n\n1. Matrix-Vector Multiplication\n- Straightforward implementation- Easy to understand\n- Computationally expensive for large grids due to dense matrix operations\n\n\n2. Sparse Matrix-Vector Multiplication\n- More memory-efficient than dense matrices- Potentially faster for large grids\n- Requires familiarity with sparse matrix operations\n\n\n3. Vectorized Operations (NumPy)\n- Avoids explicit matrix construction- Can be more memory-efficient\n- Requires understanding of array broadcasting and slicing\n\n\n4. Vectorized Operations (JAX)\n- Combines the benefits of vectorized operations with JAX’s JIT compilation for potential performance improvements\n- Requires familiarity with JAX"
  },
  {
    "objectID": "posts/16BHW4/index.html#basic-setting-up",
    "href": "posts/16BHW4/index.html#basic-setting-up",
    "title": "PIC16B HW4",
    "section": "",
    "text": "N = 101\nepsilon = 0.2\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport time\nimport jax\nfrom jax.experimental import sparse\nimport jax.numpy as jnp\nfrom jax import jit\nimport inspect\n\n\n# This is the initial condition\n# Put 1 unit of heat at midpoint\nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\nplt.imshow(u0)"
  },
  {
    "objectID": "posts/16BHW4/index.html#approach-1-simulation-with-matrix-multiplication",
    "href": "posts/16BHW4/index.html#approach-1-simulation-with-matrix-multiplication",
    "title": "PIC16B HW4",
    "section": "",
    "text": "We will first uses matrix-vector multiplication to simulate the heat diffusion in the 2D space. The vector here is created by flattening the current solution: \\(u_k^{i,j}\\)\nEach iteration of the update is given by:\n\ndef advance_time_matvecmul(A, u, epsilon):\n    \"\"\"Advances the simulation by one timestep, via matrix-vector multiplication\n    Args:\n        A: The 2d finite difference matrix, N^2 x N^2.\n        u: N x N grid state at timestep k.\n        epsilon: stability constant.\n\n    Returns:\n        N x N Grid state at timestep k+1.\n    \"\"\"\n    N = u.shape[0]\n    u = u + epsilon * (A @ u.flatten()).reshape((N, N))\n    return u\n\nWe actually view \\(u_k^{i,j}\\) as the element with index N x i + j. The matrix A has the size of N2xN2, without all zeros or all zero columns\n\n#The corresponding matrix A:\nn = N * N\ndiagonals = [-4 * np.ones(n), np.ones(n-1), np.ones(n-1), np.ones(n-N), np.ones(n-N)]\ndiagonals[1][(N-1)::N] = 0\ndiagonals[2][(N-1)::N] = 0\nA = np.diag(diagonals[0]) + np.diag(diagonals[1], 1) + np.diag(diagonals[2], -1) + np.diag(diagonals[3], N) + np.diag(diagonals[4], -N)\n\nWe will define a function get_A(N), which takes the value “N” as the argument and returns the corresponding matrix A in heat_equation.py (put the function you defined in a py. file).\n\ndef get_A(N):\n    \"\"\"The function get_A(N) takes the value N as the argument and returns the output matrix A with matrix multiplication\n    \"\"\"\n    n = N * N\n    diagonals = [-4 * np.ones(n), np.ones(n-1), np.ones(n-1), np.ones(n-N), np.ones(n-N)]\n    diagonals[1][(N-1)::N] = 0\n    diagonals[2][(N-1)::N] = 0\n    A = np.diag(diagonals[0]) + np.diag(diagonals[1], 1) + np.diag(diagonals[2], -1) + np.diag(diagonals[3], N) + np.diag(diagonals[4], -N)\n    return A\n\nThen with the get_A() and advance_time_matvecmul(), we run the code for 2700 iterations. We introduce time module previously in import libraries to help us obvserve how long does it takes.\n\nu = [u0] # Initialize a list 'u' with the initial grid state 'u0'\n\nA = get_A(N) # # Generate the Matrix A using the function 'get_A' with the grid size 'N'\nstart_time = time.time() # Record the starting time of the simulation\nfor i in range (1,2701): # Loop for 2700 iterations\n    # 'i' takes values from 1 to 2700 (inclusive)\n    u.append(advance_time_matvecmul(A, u[-1], epsilon)) #The updated grid state is appended to the list 'u' using the 'append' method\n\nprint(time.time() - start_time) # Calculate and print the elapsed time\n\n73.26655387878418\n\n\nWe observe this method 1 runs for 73 seconds. One thing for sure: this method runs excruciatingly slow.\nWe still need to visualize the diffusion of heat every 300 iterations. We present them in 3 by 3 grid of 2D heat maps\n\nfig, axs = plt.subplots(nrows=3, ncols=3, figsize=(12, 8))\n\"\"\"Create a figure and a 3x3 grid of subplots\n'figsize' specifies the width and height of the figure in inches\n\"\"\"\n\naxs = axs.flatten() # Flatten the 2D array of Axes objects into a 1D array\n# This makes it easier to access individual subplots using a single index\nsubplot_index = 0 # Initialize a variable to keep track of the current subplot index\n\nfor i in range(1, 2701):\n    if i % 300 == 0: # Check if the current iteration is divisible by 300\n        axs[subplot_index].imshow(u[i-1]) # Display the grid state at the current iteration minus 1 (u[i-1])\n\n        num_xticks = 6\n        xtick_locs = np.linspace(0, u[i-1].shape[1]-1, num_xticks)\n        xtick_labels = np.linspace(0, 100, num_xticks).astype(int)\n        axs[subplot_index].set_xticks(xtick_locs) # Set the locations of the x-axis ticks for the current subplot\n        axs[subplot_index].set_xticklabels(xtick_labels) # Set the labels for the x-axis ticks of the current subplot\n        axs[subplot_index].set_title(f\"Iteration={i}\")\n        \"\"\"Set the title of the current subplot\n        The title indicates the current iteration number\"\"\"\n        subplot_index += 1 # Increment the subplot index to move to the next subplot\n        if subplot_index &gt;= 9:\n            \"\"\"Check if all 9 subplots have been filled\n            If true, exit the loop using 'break'\"\"\"\n            break\n\nplt.subplots_adjust(wspace=0.3, hspace=0.3) # Adjust the spacing between subplots\nplt.show()"
  },
  {
    "objectID": "posts/16BHW4/index.html#approach-2-sparse-matrix-in-jax",
    "href": "posts/16BHW4/index.html#approach-2-sparse-matrix-in-jax",
    "title": "PIC16B HW4",
    "section": "",
    "text": "With a sparsed matrix, matrix multiplication will be much faster since the use of batched coordinate (BCOO) only take O(N^2) time for each update.\nWe will define a function get_sparse_A(N), which returns A_sp_matrix, a matrix A in a sparse format, given N in heat_equation.py. At the same time, we repeate Part 1 using get_A_sparse() and the jit-ed version of advance_time_matvecmul.\n\n# Converting initial condition to JAX array\nu0 = jnp.zeros((N, N)) # Initialize the grid to zero\nu0 = u0.at[int(N/2), int(N/2)].set(1.0) # Set the middle center element to 1.0\n\ndef get_sparse_A(N):\n    \"\"\"Construct the sparse Laplacian matrix for the 2D finite difference grid.\n        N: The size of the grid (N x N).\n        A_sparse: The sparse Laplacian matrix in BCOO format.\n    \"\"\"\n    n = N * N # Total number of grid points\n    \n    # Define the diagonals for the Laplacian matrix\n    diagonals = [-4 * jnp.ones(n), jnp.ones(n-1), jnp.ones(n-1), jnp.ones(n-N), jnp.ones(n-N)]\n    diagonals[1] = diagonals[1].at[(N-1)::N].set(0)\n    diagonals[2] = diagonals[2].at[(N-1)::N].set(0) # Adjust for the grid boundary\n\n    # Use JAX to construct the dense Laplacian matrix and convert to sparse format\n    A_dense = jnp.diag(diagonals[0]) + jnp.diag(diagonals[1], 1) + jnp.diag(diagonals[2], -1) + jnp.diag(diagonals[3], N) + jnp.diag(diagonals[4], -N)\n    A_sparse = sparse.BCOO.fromdense(A_dense) # Convert to sparse BCOO format\n    return A_sparse\n\n@jit\ndef advance_time_matvecmul(A, u, epsilon):\n    \"\"\"Advances the simulation by one timestep, via matrix-vector multiplication\n        A: The 2d finite difference matrix, N^2 x N^2.\n        u: N x N grid state at timestep k.\n        epsilon: stability constant.\n        N x N Grid state at timestep k+1.\n    \"\"\"\n    N = u.shape[0] # Extract the grid dimension from the current state\n    u = u + epsilon * (A @ u.flatten()).reshape((N, N)) # Scaled change is added to the current state u to produce the updated state\n    \"\"\"\n    Compute the update for the grid state using matrix-vector multiplication\n    u.flatten(): this converts the 2D array into a 1D array\n    A@u.flatten: @ is the matrix multiplication operator\n    epislon: scales the result of matrix-vector multiplication and subsequent reshaping\n    The result is reshaped back to an N x N grid\n    The scaled change is added to the current state u to produce the updated stat\n    \"\"\"\n    return u\n\nThen with the get_sparse_A() and jit-ed version of advance_time_matvecmul(), we run the code for 2700 iterations. Let’s observe how long does it takes.\n\n# We still introduce time module to help to calculate the elapsed time\nu = [u0]\nA = get_sparse_A(N)\nstart_time = time.time()\nfor i in range (1,2701):\n    u.append(advance_time_matvecmul(A, u[-1], epsilon))\nprint(time.time() - start_time) \n\n0.8678162097930908\n\n\nWe observe this method 2 runs for 0.867 seconds! Since method 1 runs for 73 seconds, method 2 is 84 times faster than method 1.\nNow let’s visualize the diffusion of heat every 300 iterations. We present them in 3 by 3 grid of 2D heat maps\n\n# We still use this way to plot graphs\nfig, axs = plt.subplots(nrows=3, ncols=3, figsize=(12, 8))\naxs = axs.flatten()\nsubplot_index = 0\n\nfor i in range(1, 2701):\n    if i % 300 == 0:\n        axs[subplot_index].imshow(u[i-1])\n\n        num_xticks = 6\n        xtick_locs = np.linspace(0, u[i-1].shape[1]-1, num_xticks)\n        xtick_labels = np.linspace(0, 100, num_xticks).astype(int)\n        axs[subplot_index].set_xticks(xtick_locs)\n        axs[subplot_index].set_xticklabels(xtick_labels)\n        axs[subplot_index].set_title(f\"Iteration={i}\")\n        subplot_index += 1\n        if subplot_index &gt;= 9:\n            break\n\nplt.subplots_adjust(wspace=0.3, hspace=0.3)\nplt.show()"
  },
  {
    "objectID": "posts/16BHW4/index.html#approach-3-direct-operation-with-numpy",
    "href": "posts/16BHW4/index.html#approach-3-direct-operation-with-numpy",
    "title": "PIC16B HW4",
    "section": "",
    "text": "Matrix-vector multiplication approach is not absolutely necessary in terms of computation. With vectorized array operations like np. roll(), the operations could be simpler.\nWe can then write a function advance_time_numpy(u, epsilon) that advances the solution by one timestep. We could pay zeroes to the input array to form an (N+2)x(N=2) array internally, but the argument and the returned solution should still be N x N.\n\ndef advance_time_numpy(u, epsilon):\n    \"\"\"\n    The function computes the Laplacian of the input grid 'u' via vectorized opertaions,\n    simulating a diffusion process. We use Euler method as the applied update rule.\n    \n    u: The input grid representing the current state of the system.\n    epsilon: The diffusion coefficient controlling the rate of diffusion.\n    u_next: The updated grid after one time step of the diffusion process.\n    \"\"\"\n    N = u.shape[0]  \n    \"\"\"Extract the size of the grid (assuming a square grid)\n    Pad the input array with zeros on all sides for boundary conditions\n    This ensures that the boundary values remain zero during the diffusion process\"\"\"\n    \n    u_padded = np.pad(u, 1, mode='constant', constant_values=0)\n    # Compute the Laplacian of u using vectorized operations\n    laplacian_u = (np.roll(u_padded, 1, axis=0)  # Shift the array one step down\n                   + np.roll(u_padded, -1, axis=0) \n                   + np.roll(u_padded, 1, axis=1) # Shift the array one step to the right\n                   + np.roll(u_padded, -1, axis=1) \n                   -4 * u_padded)[1:-1, 1:-1] # Extract the inner part of the array (excluding the padded boundary)\n    \"\"\" We use np.roll to shift the padded array in four directions (up, down, left, right)\n    and then sum the shifted arrays to compute the Laplacian\"\"\"\n    \n    u_next = u + epsilon * laplacian_u \n    \"\"\" Update the grid \"u\" based on the Laplacian and the diffusion coefficient 'epsilon'\n    The update rule is: u_next = u + epsilon * laplacian_u \"\"\"\n    return u_next\n\nThen with the advance_time_numpy(u,epsilon), we run the code for 2700 iterations. Let’s obvserve how long does it takes.\n\n\nu = u0\nstart_time = time.time()\nfor t in range(1, 2701):\n    u = advance_time_numpy(u, epsilon) # Update the grid state 'u' using the 'advance_time_numpy' function\n    \nprint(time.time() - start_time)\n\n0.21011614799499512\n\n\nIt only takes 0.21 s to run the method 3, method 1 takes 73s, it’s 347x faster than method 1\nNow let’s visualize the diffusion of heat every 300 iterations. We present them in 3 by 3 grid of 2D heat maps\n\nu = u0\nfig, axs = plt.subplots(3, 3, figsize=(9, 9))\naxs = axs.flatten()\nsubplot_index = 0\n\nfor i in range(1, 2701):\n    u = advance_time_numpy(u, epsilon)\n    # Update the grid state 'u' using the 'advance_time_numpy' function\n    if i % 300 == 0:\n        axs[subplot_index].imshow(u)\n        axs[subplot_index].set_title(f\"Iteration {i}\")\n\n        num_xticks = 6\n        xtick_locs = np.linspace(0, u.shape[1]-1, num_xticks)\n        xtick_labels = np.linspace(0, 100, num_xticks).astype(int)\n        axs[subplot_index].set_xticks(xtick_locs)\n        axs[subplot_index].set_xticklabels(xtick_labels)\n\n        subplot_index += 1\n\n        if subplot_index &gt;= 9:\n            break\n\nplt.tight_layout()\nplt.subplots_adjust(wspace=0.3, hspace=0.3)\nplt.show()"
  },
  {
    "objectID": "posts/16BHW4/index.html#approach-4-with-jax",
    "href": "posts/16BHW4/index.html#approach-4-with-jax",
    "title": "PIC16B HW4",
    "section": "",
    "text": "Now, we will use jax to do the similar using just-in-time compilation.\nWe will now define a function advance_time_jax(u, epsilon) but without using (sparse) matrix multiplication routines. It will be simple if we use the function advance_time_numpy() as the starting point. Keep in mind that jax does not support index assignment.”\n\nimport jax.numpy as jnp\nfrom jax import jit\n\n@jit\ndef advance_time_jax(u, epsilon):\n    \"\"\"\n    Advances the simulation state by one time step using the Laplacian operator and Euler's method.\n    \n    This function computes the Laplacian of the input array 'u' using direct JAX operations for faster execution.\n    It then updates the simulation state based on the computed Laplacian and a given 'epsilon' value, which\n    represents the time step size or the diffusion coefficient.\n\n    \"\"\"\n    N = u.shape[0]\n    \"\"\"Pad the input array with zeros to handle edge conditions\n    The 'mode' argument specifies the padding mode, which is set to 'constant'\n    The 'constant_values' argument specifies the value to be used for padding, which is set to 0.\"\"\"\n    u_padded = jnp.pad(u, 1, mode='constant', constant_values=0)\n\n    # Compute the Laplacian using vectorized operations\n    laplacian_u = (jnp.roll(u_padded, 1, axis=0) + jnp.roll(u_padded, -1, axis=0) +\n                   jnp.roll(u_padded, 1, axis=1) + jnp.roll(u_padded, -1, axis=1) -\n                   4 * u_padded)[1:-1, 1:-1]\n    \n   # Update the simulation state based on the computed Laplacian and the given 'epsilon' value.\n    u_next = u + epsilon * laplacian_u\n    return u_next\n\nThen with the advance_time_jax(u,epsilon), we run the code for 2700 iterations. Let’s obvserve how long does it takes.\n\nu = u0\nstart_time = time.time()\nfor i in range(1, 2701):\n    u = advance_time_jax(u, epsilon)  # Update the grid state 'u' using the 'advance_time_jax' function\nprint(time.time() - start_time)\n\n0.10694193840026855\n\n\nIt only takes 0.10 s to run the method 4. We know method 3 takes 0.21s, method 4 is about 2x faster than method 3.\nNow let’s visualize the diffusion of heat every 300 iterations. We present them in 3 by 3 grid of 2D heat maps\n\nu = u0\nfig, axs = plt.subplots(3, 3, figsize=(9, 9))\naxs = axs.flatten()\nsubplot_index = 0\n\nfor i in range(1, 2701):\n    u = advance_time_jax(u, epsilon) # # Update the grid state 'u' using the 'advance_time_jax' function\n    if i % 300 == 0:\n        axs[subplot_index].imshow(u)\n        axs[subplot_index].set_title(f\"Iteration {i}\")\n\n        num_xticks = 6\n        xtick_locs = np.linspace(0, u.shape[1]-1, num_xticks)\n        xtick_labels = np.linspace(0, 100, num_xticks).astype(int)\n        axs[subplot_index].set_xticks(xtick_locs)\n        axs[subplot_index].set_xticklabels(xtick_labels)\n\n        num_yticks = 6\n        ytick_locs = np.linspace(0, u.shape[0]-1, num_yticks)\n        ytick_labels = np.linspace(0, 100, num_yticks).astype(int)\n        axs[subplot_index].set_yticks(ytick_locs)\n        axs[subplot_index].set_yticklabels(ytick_labels)\n\n        subplot_index += 1\n\n        if subplot_index &gt;= 9:\n            break\n\nplt.tight_layout()\nplt.subplots_adjust(wspace=0.3, hspace=0.3)\nplt.show()"
  },
  {
    "objectID": "posts/16BHW4/index.html#models-comparison",
    "href": "posts/16BHW4/index.html#models-comparison",
    "title": "PIC16B HW4",
    "section": "",
    "text": "Now let’s compare the performances of four models. Let’s summarize which one is fastest, and which one is easier for us to write.\n\n\n\nModel\nTime\n\n\n\n\nModel 1\n73s\n\n\nModel 2\n0.86s\n\n\nModel 3\n0.21s\n\n\nModel 4\n0.10s\n\n\n\nPerformance Comparison: The matrix multiplication approach (Approach 1) is the slowest among the given methods. Using a sparse matrix with JAX (Approach 2) provides a significant speedup, approximately 80 times faster than the matrix multiplication approach. Utilizing np.roll() (Approach 3) simplifies the computation for the CPU by eliminating the heat points around and adding them together, resulting in improved performance compared to the matrix multiplication approach. JAX (Approach 4) generally offers better performance compared to NumPy.\nSpeed of Implementation: Method 4 is the fastest and the most performant\nEase of Implementation: Method one appears to be the easiest to implement since the code is already provided. The other approaches may require additional understanding of sparse matrices, JIT compilation, and vectorized operations.\n\n\n\n\n\n\n\n\nApproach\nPros\nCons\n\n\n\n\n1. Matrix-Vector Multiplication\n- Straightforward implementation- Easy to understand\n- Computationally expensive for large grids due to dense matrix operations\n\n\n2. Sparse Matrix-Vector Multiplication\n- More memory-efficient than dense matrices- Potentially faster for large grids\n- Requires familiarity with sparse matrix operations\n\n\n3. Vectorized Operations (NumPy)\n- Avoids explicit matrix construction- Can be more memory-efficient\n- Requires understanding of array broadcasting and slicing\n\n\n4. Vectorized Operations (JAX)\n- Combines the benefits of vectorized operations with JAX’s JIT compilation for potential performance improvements\n- Requires familiarity with JAX"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PIC16B",
    "section": "",
    "text": "PIC16B HW6\n\n\n\n\n\n\nweek 10\n\n\nhw6\n\n\n\n\n\n\n\n\n\nMar 11, 2024\n\n\nManshu Huang\n\n\n\n\n\n\n\n\n\n\n\n\nPIC16B HW5\n\n\n\n\n\n\nweek 8\n\n\nhw5\n\n\n\n\n\n\n\n\n\nMar 4, 2024\n\n\nManshu Huang\n\n\n\n\n\n\n\n\n\n\n\n\nPIC16B HW4\n\n\n\n\n\n\nweek 6\n\n\nhw4\n\n\n\n\n\n\n\n\n\nFeb 16, 2024\n\n\nManshu Huang\n\n\n\n\n\n\n\n\n\n\n\n\nPIC16B HW3\n\n\n\n\n\n\nweek 6\n\n\nhw2\n\n\n\n\n\n\n\n\n\nFeb 14, 2024\n\n\nManshu Huang\n\n\n\n\n\n\n\n\n\n\n\n\nPIC 16B HW2\n\n\n\n\n\n\nweek 5\n\n\nhw2\n\n\n\n\n\n\n\n\n\nFeb 12, 2024\n\n\nManshu Huang\n\n\n\n\n\n\n\n\n\n\n\n\nPIC 16B HW1\n\n\n\n\n\n\nweek 4\n\n\nhw1\n\n\n\n\n\n\n\n\n\nFeb 5, 2024\n\n\nManshu Huang\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nManshu Huang\n\n\n\n\n\n\nNo matching items"
  }
]